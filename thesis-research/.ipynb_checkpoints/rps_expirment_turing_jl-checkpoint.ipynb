{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook presents an expriment of Theory Of Mind on Rock Paper Scissors\n",
    "### The goal is to research and devlop a method for generic multi-agent games using Theory of Mind modeling.\n",
    "### We will introduce the tradeoff between choosing optimal move and staying unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "using Turing, StatsPlots, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model\n",
    "1) prior on opponents <br>\n",
    "2) history <br>\n",
    "3) observation - history + prior => postrior <br> \n",
    "4) counter policy - 1) beating the next round 2) confuse the opponent (noise) <br>\n",
    "5) depth > 0 -> no history <br>\n",
    "6) sample action from the counter policy<br>\n",
    "7) optimal noise parameter <br>\n",
    "8) implment Bob and Alice <br>\n",
    "9) conditioning as rejection sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computer_counter_policy\n",
    "Given infered information about the opponent.<br>\n",
    "Decide how to exploit it.<br>\n",
    "In the future we will optimize those parameters, to make the agent's move less predictible <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_counter_policy (generic function with 3 methods)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_counter_policy(opp_next_move, noise_factor = 0.1)\n",
    "    counter_move_dict = Dict(1 => 2, 2 => 3, 3 => 1)\n",
    "    counter_move = counter_move_dict[opp_next_move]\n",
    "    policy = ones(3) * (noise_factor / 2)\n",
    "    policy[counter_move] = 1 - noise_factor\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different counter policy computing.\n",
    "not optimal! will be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_counter_policy (generic function with 3 methods)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_counter_policy(opp_alpha_1, opp_alpha_2, opp_alpha_3)\n",
    "    normalize_factor = opp_alpha_1 + opp_alpha_2 + opp_alpha_3\n",
    "    return [opp_alpha_3 / normalize_factor, opp_alpha_1 / normalize_factor, opp_alpha_2 / normalize_factor]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given history we want to know the distributin of the moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_distribution_from_history (generic function with 1 method)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_distribution_from_history(actions_history)\n",
    "    count = Dict(1 => 1, 2 => 1, 3 => 1)\n",
    "    for i in 1:(length(actions_history)-1)\n",
    "        opponent_action = actions_history[i]\n",
    "        count[opponent_action] = count[opponent_action] + 1\n",
    "    end\n",
    "    return [v for (k,v) in count]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The core of the notebook\n",
    "### Modeling Theory of Mind in Rock Paper Sciors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model of agent as universal Probalistic Model\n",
    "### gets some parmeters:\n",
    "<ul>\n",
    "    <li> opponent agent - representation of our belief on the opponent model (Turing.jl model)\n",
    "    <li> my history - list of all moves this player done (At this point unused)\n",
    "    <li> opponent history - list of all moves the opponent done, used to estimate a prior on the moves\n",
    "    <li> depth - the depth we want the agent will dive modeling the mind of opponent agent\n",
    "</ul>        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent (generic function with 5 methods)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function agent(opponent_agent, my_history, opponent_history, depth = 0, discrete_sampler = PG, discrete_sampler_hyper_param=5, num_of_iterations=5)\n",
    "        if depth == 0\n",
    "            opponent_history_distribution = compute_distribution_from_history(opponent_history)\n",
    "            opp_alpha_1, opp_alpha_2, opp_alpha_3 = opponent_history_distribution\n",
    "            counter_policy_prior_params = TArray(compute_counter_policy(opp_alpha_1, opp_alpha_2, opp_alpha_3))\n",
    "        else\n",
    "            opp_action_chain = sample(opponent_agent(agent, opponent_history, my_history, depth-1), discrete_sampler(discrete_sampler_hyper_param), num_of_iterations, progress=true);\n",
    "            opp_next_move = round(Int64, mean(opp_action_chain[:\"next_move\"]))\n",
    "            counter_policy_prior_params = TArray(compute_counter_policy(opp_next_move))\n",
    "        end\n",
    "        counter_policy ~ Dirichlet(counter_policy_prior_params)\n",
    "        next_move ~ Categorical(counter_policy)\n",
    "        return round(Int64, next_move)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-dimensional AxisArray{Float64,2,...} with axes:\n",
       "    :iter, 1:1:1\n",
       "    :chain, 1:1\n",
       "And data, a 1×1 Array{Float64,2}:\n",
       " 2.0"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = sample(agent(agent, ones(10) , 2* ones(10)), PG(1), 1)\n",
    "chain[:\"next_move\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple aux function that return the most common sample - majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "most_common (generic function with 1 method)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[35mSampling:  30%|█████████████                            |  ETA: 0:00:01\u001b[39m"
     ]
    }
   ],
   "source": [
    "function most_common(samples)\n",
    "    count = Dict(1 => 0, 2 => 0, 3 => 0)\n",
    "    for i in 1:length(samples)\n",
    "        count[samples[i]] += 1\n",
    "    end\n",
    "    max_k, max_v = -1 , -1\n",
    "    for (k, v) in count\n",
    "        if v > max_v\n",
    "            max_k , max_v = k, v\n",
    "        end\n",
    "    end\n",
    "    return max_k\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This method is used for the simulation, each model make a move at the end of this function, as a result of reasoning of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "move (generic function with 6 methods)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function move(agent, other_agent, my_history, other_agent_history, my_depth=1)\n",
    "    other_agent_history = length(other_agent_history) > 0 ? other_agent_history : [1]\n",
    "    my_history = length(my_history) > 0 ? my_history : [1]\n",
    "    my_history = Array{Int}(my_history)\n",
    "    other_agent_history = Array{Int}(other_agent_history)\n",
    "    chain = sample(agent(other_agent, my_history, other_agent_history, my_depth), PG(5), 5, progress = true)\n",
    "    return most_common(chain[:\"next_move\"])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[2, 3]\n",
      "[1, 2, 3]\n",
      "[2, 3, 2]\n",
      "[1, 2, 2, 1]\n",
      "[2, 3, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move(agent, agent, [1, 2], [2 , 3])\n",
    "move(agent, agent, [1, 2, 3], [2 , 3, 2])\n",
    "move(agent, agent, [1, 2, 2, 1], [2 , 3, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game simulation , given two agent models , depth params, let them play num of simulation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "game (generic function with 3 methods)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function game(first_player_depth = 1, second_player_depth = 1)\n",
    "    first_player = agent\n",
    "    second_player = agent\n",
    "    num_of_simulations = 100\n",
    "    first_player_history = []\n",
    "    second_player_history = []\n",
    "    for i in 1:num_of_simulations\n",
    "        m1 = move(first_player, second_player, first_player_history, second_player_history, first_player_depth)\n",
    "#         println(\"player1 choose $m1\")\n",
    "        push!(first_player_history, m1)\n",
    "        m2 = move(second_player, first_player, second_player_history, first_player_history, second_player_depth)\n",
    "#         println(\"player2 choose $m2\")\n",
    "        push!(second_player_history, m2)\n",
    "#         println(\"in simulation $i first player chose $m1 second player chose $m2\")\n",
    "    end\n",
    "    return first_player_history, second_player_history\n",
    " end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score (generic function with 2 methods)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function score(history)\n",
    "    first_player_history, second_player_history = history\n",
    "    first_wins = 0\n",
    "    ties = 0\n",
    "    second_wins = 0\n",
    "    wins = Dict(1 => 3, 2 => 1, 3 => 2)\n",
    "    for i in 1:length(first_player_history)\n",
    "        if wins[first_player_history[i]] == second_player_history[i]\n",
    "            first_wins += 1\n",
    "        elseif wins[second_player_history[i]] == first_player_history[i]\n",
    "            second_wins += 1\n",
    "        else\n",
    "            ties += 1\n",
    "        end\n",
    "    end\n",
    "    return first_wins, ties, second_wins\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "display_score (generic function with 2 methods)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function display_score(score)\n",
    "    num_of_wins_first, num_of_ties, num_of_wins_second = score\n",
    "    println(\"first player won: $num_of_wins_first\")\n",
    "    println(\"second player won: $num_of_wins_second\") \n",
    "    println(\"ties: $num_of_ties\") \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both player plays without theory of mind - equivilent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first player won: 1\n",
      "second player won: 1\n",
      "ties: 98\n"
     ]
    }
   ],
   "source": [
    "display_score(score(game(0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd player plays with one level of theory of mind -> 2nd wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first player won: 15\n",
      "second player won: 28\n",
      "ties: 57\n"
     ]
    }
   ],
   "source": [
    "display_score(score(game(0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st player plays with one level of theory of mind -> 1st wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first player won: 27\n",
      "second player won: 17\n",
      "ties: 56\n"
     ]
    }
   ],
   "source": [
    "display_score(score(game(1, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both players play with one level of theory of mind -> preety much equivelent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first player won: 23\n",
      "second player won: 20\n",
      "ties: 57\n"
     ]
    }
   ],
   "source": [
    "display_score(score(game(1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
