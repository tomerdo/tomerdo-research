{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "using Turing, StatsPlots, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent (generic function with 6 methods)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare our Turing model.\n",
    "@model function agent(opponent_agent, my_action, opponent_action, depth=0, discrete_sampler = PG, discrete_sampler_hyper_param=10, num_of_iterations=10)\n",
    "    if depth > 0 \n",
    "        opp_action_chain = sample(opponent_agent(agent, opponent_action, my_action, depth-1), discrete_sampler(discrete_sampler_hyper_param), num_of_iterations, progress=false);\n",
    "        opp_alpha_1 = mean(opp_action_chain[:\"alpha[1]\"])\n",
    "        opp_alpha_2 = mean(opp_action_chain[:\"alpha[2]\"])\n",
    "        opp_alpha_3 = 1 - opp_alpha_1 - opp_alpha_2\n",
    "        alpha ~ Dirichlet([opp_alpha_3, opp_alpha_1 , opp_alpha_2])\n",
    "    else\n",
    "        # Our prior belief about the probability of RPS.\n",
    "        alpha ~ Dirichlet(ones(3)/3)\n",
    "    end\n",
    "    my_action ~ Categorical(vec(alpha))\n",
    "    return my_action\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent (generic function with 5 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare our Turing model.\n",
    "@model function agent(opponent_agent, my_history, opponent_history, depth=0, discrete_sampler = PG, discrete_sampler_hyper_param=1, num_of_iterations=1)\n",
    "    # Our prior belief about the probability of RPS.\n",
    "    alpha ~ Dirichlet(ones(3)/3)\n",
    "    for i in 1:length(my_history)\n",
    "        println(\"$(i): depth: $(depth)\")\n",
    "        my_action = my_history[i]\n",
    "        opponent_action = opponent_history[i]\n",
    "        if depth > 0\n",
    "            println(\"before sample i is $(i) depth is $(depth)\")\n",
    "            opp_action_chain = sample(opponent_agent(agent, opponent_history, my_history, depth-1), discrete_sampler(discrete_sampler_hyper_param), num_of_iterations, progress=true);\n",
    "            println(\"after sample i is $(i) depth is $(depth)\")\n",
    "            opp_alpha_1 = mean(opp_action_chain[:\"alpha[1]\"])\n",
    "            opp_alpha_2 = mean(opp_action_chain[:\"alpha[2]\"])\n",
    "            opp_alpha_3 = 1 - opp_alpha_1 - opp_alpha_2\n",
    "            counter_opponent_policy = [opp_alpha_3, opp_alpha_1 , opp_alpha_2]\n",
    "            counter_opponent_policy ~ Dirichlet(alpha)\n",
    "        end\n",
    "        println(\"ended i: $(i) observation depth is $(depth)\")\n",
    "        my_action ~ Categorical(alpha)\n",
    "    end\n",
    "    println(\"ended computation on $(length(my_history)) and $(length(opponent_history))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_action = 1\n",
    "opponent_action = 1\n",
    "my_depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: depth: 1\n",
      "before sample i is 1 depth is 1\n",
      "1: depth: 0\n",
      "ended i: 1 observation depth is 0\n",
      "ended computation on 1 and 1\n",
      "1: depth: 0\n",
      "ended i: 1 observation depth is 0\n",
      "ended computation on 1 and 1\n",
      "after sample i is 1 depth is 1\n",
      "ended i: 1 observation depth is 1\n",
      "ended computation on 1 and 1\n",
      "1: depth: 1\n",
      "before sample i is 1 depth is 1\n",
      "1: depth: 0\n",
      "ended i: 1 observation depth is 0\n",
      "ended computation on 1 and 1\n",
      "1: depth: 0\n",
      "ended i: 1 observation depth is 0\n",
      "ended computation on 1 and 1\n",
      "after sample i is 1 depth is 1\n",
      "ended i: 1 observation depth is 1\n",
      "ended computation on 1 and 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chains MCMC chain (1×9×1 Array{Float64,3}):\n",
       "\n",
       "Log evidence      = 0.0\n",
       "Iterations        = 1:1\n",
       "Thinning interval = 1\n",
       "Chains            = 1\n",
       "Samples per chain = 1\n",
       "parameters        = alpha[1], alpha[2], alpha[3], counter_opponent_policy[1], counter_opponent_policy[2], counter_opponent_policy[3], my_action\n",
       "internals         = logevidence, lp\n",
       "\n",
       "Summary Statistics\n",
       " \u001b[1m                 parameters \u001b[0m \u001b[1m    mean \u001b[0m \u001b[1m     std \u001b[0m \u001b[1m naive_se \u001b[0m \u001b[1m    mcse \u001b[0m \u001b[1m     es\u001b[0m ⋯\n",
       " \u001b[90m                     Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m Missing \u001b[0m \u001b[90m Missin\u001b[0m ⋯\n",
       "\n",
       "                    alpha[1]    0.0063       NaN        NaN   missing   missin ⋯\n",
       "                    alpha[2]    0.5864       NaN        NaN   missing   missin ⋯\n",
       "                    alpha[3]    0.4072       NaN        NaN   missing   missin ⋯\n",
       "  counter_opponent_policy[1]    0.0000       NaN        NaN   missing   missin ⋯\n",
       "  counter_opponent_policy[2]    0.9785       NaN        NaN   missing   missin ⋯\n",
       "  counter_opponent_policy[3]    0.0215       NaN        NaN   missing   missin ⋯\n",
       "                   my_action    3.0000       NaN        NaN   missing   missin ⋯\n",
       "\u001b[31m                                                               2 columns omitted\u001b[0m\n",
       "\n",
       "Quantiles\n",
       " \u001b[1m                 parameters \u001b[0m \u001b[1m    2.5% \u001b[0m \u001b[1m   25.0% \u001b[0m \u001b[1m   50.0% \u001b[0m \u001b[1m   75.0% \u001b[0m \u001b[1m   97.5%\u001b[0m ⋯\n",
       " \u001b[90m                     Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64\u001b[0m ⋯\n",
       "\n",
       "                    alpha[1]    0.0063    0.0063    0.0063    0.0063    0.0063 ⋯\n",
       "                    alpha[2]    0.5864    0.5864    0.5864    0.5864    0.5864 ⋯\n",
       "                    alpha[3]    0.4072    0.4072    0.4072    0.4072    0.4072 ⋯\n",
       "  counter_opponent_policy[1]    0.0000    0.0000    0.0000    0.0000    0.0000 ⋯\n",
       "  counter_opponent_policy[2]    0.9785    0.9785    0.9785    0.9785    0.9785 ⋯\n",
       "  counter_opponent_policy[3]    0.0215    0.0215    0.0215    0.0215    0.0215 ⋯\n",
       "                   my_action    3.0000    3.0000    3.0000    3.0000    3.0000 ⋯\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = sample(agent(agent, [my_action], [opponent_action], my_depth), PG(1), 1 , progress = true)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maximum_likelihood_action (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function maximum_likelihood_action(list_of_pairs)\n",
    "    max_key = -1\n",
    "    max_value = -1\n",
    "    for element in list_of_pairs\n",
    "        key = element[1]\n",
    "        value = element[2]\n",
    "        if value > max_value\n",
    "            max_value = value\n",
    "            max_key = key\n",
    "        end\n",
    "    end\n",
    "    println(\"maximum likelihood is $((max_key, max_value))\")\n",
    "    return (max_key, max_value)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "move (generic function with 2 methods)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function move(agent, other_agent, my_history, other_agent_history, my_depth=1)\n",
    "    other_agent_history = length(other_agent_history) > 0 ? other_agent_history : [1]\n",
    "    my_history = length(my_history) > 0 ? my_history : [1]\n",
    "    chain = sample(agent(other_agent, my_history[1], other_agent_history[1], my_depth), PG(1), 1, progress = false)\n",
    "    println(\"chain computation is ended\")\n",
    "    alpha_1 = (1, mean(chain[:\"alpha[1]\"]))\n",
    "    alpha_2 = (2, mean(chain[:\"alpha[2]\"]))\n",
    "    alpha_3 = (3, 1 - alpha_1[2] - alpha_2[2])\n",
    "    return maximum_likelihood_action([alpha_1, alpha_2, alpha_3])[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: depth: 1\n",
      "before sample i is 1 depth is 1\n",
      "1: depth: 0\n",
      "ended i: 1 observation depth is 0\n",
      "ended computation on 1 and 1\n",
      "1: depth: 0\n",
      "ended i: 1 observation depth is 0\n",
      "ended computation on 1 and 1\n",
      "after sample i is 1 depth is 1\n",
      "ended i: 1 observation depth is 1\n",
      "ended computation on 1 and 1\n",
      "1: depth: 1\n",
      "before sample i is 1 depth is 1\n",
      "1: depth: 0\n",
      "ended i: 1 observation depth is 0\n",
      "ended computation on 1 and 1\n",
      "1: depth: 0\n",
      "ended i: 1 observation depth is 0\n",
      "ended computation on 1 and 1\n",
      "after sample i is 1 depth is 1\n",
      "ended i: 1 observation depth is 1\n",
      "ended computation on 1 and 1\n",
      "chain computation is ended\n",
      "maximum likelihood is (3, 0.9666186395714202)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move(agent, agent, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "game (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function game()\n",
    "    first_player = agent\n",
    "    second_player = agent\n",
    "    num_of_simulations = 10\n",
    "    first_player_history = []\n",
    "    second_player_history = []\n",
    "    first_player_depth = 1\n",
    "    second_player_depth = 0\n",
    "    for i in 1:num_of_simulations\n",
    "        m1 = move(first_player, second_player, first_player_history, second_player_history, first_player_depth)\n",
    "        println(\"player1 choose $m1\")\n",
    "        push!(first_player_history, m1)\n",
    "        m2 = move(second_player, first_player, second_player_history, first_player_history, second_player_depth)\n",
    "        println(\"player2 choose $m2\")\n",
    "        push!(second_player_history, m2)\n",
    "        println(\"in simulation $i first player chose $m1 second player chose $m2\")\n",
    "    end\n",
    "    return first_player_history, second_player_history\n",
    " end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain computation is ended\n",
      "maximum likelihood is (2, 0.9995476223214993)\n",
      "player1 choose 2\n",
      "chain computation is ended\n",
      "maximum likelihood is (1, 0.9722738970433269)\n",
      "player2 choose 1\n",
      "in simulation 1 first player chose 2 second player chose 1\n",
      "chain computation is ended\n",
      "maximum likelihood is (2, 0.9801800371722309)\n",
      "player1 choose 2\n",
      "chain computation is ended\n",
      "maximum likelihood is (3, 0.9631776816937098)\n",
      "player2 choose 3\n",
      "in simulation 2 first player chose 2 second player chose 3\n",
      "chain computation is ended\n",
      "maximum likelihood is (2, 0.9982746657226768)\n",
      "player1 choose 2\n",
      "chain computation is ended\n",
      "maximum likelihood is (2, 0.5868857049995163)\n",
      "player2 choose 2\n",
      "in simulation 3 first player chose 2 second player chose 2\n",
      "chain computation is ended\n",
      "maximum likelihood is (2, 0.907244803100407)\n",
      "player1 choose 2\n",
      "chain computation is ended\n",
      "maximum likelihood is (3, 0.9583478195206196)\n",
      "player2 choose 3\n",
      "in simulation 4 first player chose 2 second player chose 3\n",
      "chain computation is ended\n",
      "maximum likelihood is (2, 0.721631677378695)\n",
      "player1 choose 2\n",
      "chain computation is ended\n",
      "maximum likelihood is (3, 0.8196235207822419)\n",
      "player2 choose 3\n",
      "in simulation 5 first player chose 2 second player chose 3\n",
      "chain computation is ended\n",
      "maximum likelihood is (3, 0.8323326606081162)\n",
      "player1 choose 3\n",
      "chain computation is ended\n",
      "maximum likelihood is (1, 0.9780659712679559)\n",
      "player2 choose 1\n",
      "in simulation 6 first player chose 3 second player chose 1\n",
      "chain computation is ended\n",
      "maximum likelihood is (1, 0.534680634960536)\n",
      "player1 choose 1\n",
      "chain computation is ended\n",
      "maximum likelihood is (2, 0.9140884948557275)\n",
      "player2 choose 2\n",
      "in simulation 7 first player chose 1 second player chose 2\n",
      "chain computation is ended\n",
      "maximum likelihood is (2, 0.9790761514724974)\n",
      "player1 choose 2\n",
      "chain computation is ended\n",
      "maximum likelihood is (2, 0.8909610959925417)\n",
      "player2 choose 2\n",
      "in simulation 8 first player chose 2 second player chose 2\n",
      "chain computation is ended\n",
      "maximum likelihood is (2, 0.9058150202892957)\n",
      "player1 choose 2\n",
      "chain computation is ended\n",
      "maximum likelihood is (1, 0.9612426849029302)\n",
      "player2 choose 1\n",
      "in simulation 9 first player chose 2 second player chose 1\n",
      "chain computation is ended\n",
      "maximum likelihood is (2, 0.9768800602636778)\n",
      "player1 choose 2\n",
      "chain computation is ended\n",
      "maximum likelihood is (1, 0.7716869592998048)\n",
      "player2 choose 1\n",
      "in simulation 10 first player chose 2 second player chose 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Any[2, 2, 2, 2, 2, 3, 1, 2, 2, 2], Any[1, 3, 2, 3, 3, 1, 2, 2, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
