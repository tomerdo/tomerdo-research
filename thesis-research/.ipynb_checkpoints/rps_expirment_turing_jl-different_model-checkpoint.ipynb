{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook presents an expriment of Theory Of Mind on Rock Paper Scissors\n",
    "### The goal is to research and devlop a method for generic multi-agent games using Theory of Mind modeling.\n",
    "### We will introduce the tradeoff between choosing optimal move and staying unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "using Turing, StatsPlots, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computer_counter_policy\n",
    "Given infered information about the opponent.<br>\n",
    "Decide how to exploit it.<br>\n",
    "In the future we will optimize those parameters, to make the agent's move less predictible <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different counter policy computing.\n",
    "not optimal! will be changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given history we want to know the distributin of the moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The core of the notebook\n",
    "### Modeling Theory of Mind in Rock Paper Sciors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model of agent as universal Probalistic Model\n",
    "### gets some parmeters:\n",
    "<ul>\n",
    "    <li> opponent agent - representation of our belief on the opponent model (Turing.jl model)\n",
    "    <li> my history - list of all moves this player done (At this point unused)\n",
    "    <li> opponent history - list of all moves the opponent done, used to estimate a prior on the moves\n",
    "    <li> depth - the depth we want the agent will dive modeling the mind of opponent agent\n",
    "</ul>        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p_deceive = 0.5\n",
    "\n",
    "def best_move(opp_move):\n",
    "    best_moves = {1: 2, 2:3, 3:1}\n",
    "    move = best_moves[opp_move]\n",
    "    deceive ~ Bernoulli(p_deceive)\n",
    "    if deceive:\n",
    "        move = opp_move\n",
    "    return move\n",
    "  \n",
    "if depth == 0:\n",
    "    beta_opp ~ Dirichlet(1, 1, 1)\n",
    "    for opp_move in opp_history:\n",
    "        opp_move ~ Categorical(beta_opp) # observe\n",
    "    next_opp_move ~ Categorical(beta_opp) # sample\n",
    "else:\n",
    "    next_opp_move = sample(agent(depth=n-1)) # if you use MH - you need to burn samples in the chain first\n",
    "move = best_move(next_opp_move)\n",
    "return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "best_move (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function best_move(opp_move)\n",
    "    best_moves = Dict(1 => 2, 2 => 3, 3 => 1)\n",
    "    move = best_moves[opp_move]\n",
    "    println(move)\n",
    "#     p_deceive = 0.001\n",
    "#     print(p_deceive)\n",
    "#     deceive ~ Bernoulli(p_deceive)\n",
    "#     print(deceive)\n",
    "#     if deceive\n",
    "#         move = opp_move\n",
    "#     end\n",
    "    return move\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent (generic function with 5 methods)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function agent(opponent_agent, my_history, opponent_history, depth = 1, discrete_sampler = PG, discrete_sampler_hyper_param=1, num_of_iterations=1)\n",
    "    if depth == 0\n",
    "        println(\"in depth 0\")\n",
    "        beta_opp ~ Dirichlet([1, 1, 1])\n",
    "        println(\"length is \", length(opponent_history))\n",
    "        for opp_move in opponent_history\n",
    "            opp_move ~ Categorical(beta_opp) # observe\n",
    "        end\n",
    "    next_opp_move ~ Categorical(beta_opp) # sample\n",
    "    println(\"next opp move is  \", next_opp_move)\n",
    "    else\n",
    "        opponent_model = opponent_agent(agent, opponent_history, my_history, depth-1)\n",
    "        print(\"before sampling\")\n",
    "        next_opp_move_chain  = sample(opponent_model, discrete_sampler(discrete_sampler_hyper_param), num_of_iterations, progress=true); # if you use MH - you need to burn samples in the chain first\n",
    "        result = generated_quantities(opponent_model, next_opp_move_chain)\n",
    "#         print(\"result is \" , display(result))\n",
    "        # last sample\n",
    "        println(\"chain is ready and it is \", display(next_opp_move_chain))\n",
    "        println(\"result is ready and it is \", result)\n",
    "        next_opp_move = last(result)\n",
    "        println(\"next opp is \" ,next_opp_move)\n",
    "    end\n",
    "    move = best_move(next_opp_move)\n",
    "    println(\"move is \", move)\n",
    "    return move\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the infece is about the opponent model and not my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before samplingin depth 0\n",
      "length is 10\n",
      "next opp move is  2\n",
      "3\n",
      "move is 3\n",
      "in depth 0\n",
      "length is 10\n",
      "next opp move is  1\n",
      "2\n",
      "move is 2\n",
      "in depth 0\n",
      "length is 10\n",
      "next opp move is  3\n",
      "1\n",
      "move is 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chains MCMC chain (1×7×1 Array{Float64,3}):\n",
       "\n",
       "Log evidence      = 0.0\n",
       "Iterations        = 1:1\n",
       "Thinning interval = 1\n",
       "Chains            = 1\n",
       "Samples per chain = 1\n",
       "parameters        = beta_opp[1], beta_opp[2], beta_opp[3], next_opp_move, opp_move\n",
       "internals         = logevidence, lp\n",
       "\n",
       "Summary Statistics\n",
       " \u001b[1m    parameters \u001b[0m \u001b[1m    mean \u001b[0m \u001b[1m     std \u001b[0m \u001b[1m naive_se \u001b[0m \u001b[1m    mcse \u001b[0m \u001b[1m     ess \u001b[0m \u001b[1m    rhat \u001b[0m\n",
       " \u001b[90m        Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m Missing \u001b[0m \u001b[90m Missing \u001b[0m \u001b[90m Missing \u001b[0m\n",
       "\n",
       "    beta_opp[1]    0.5512       NaN        NaN   missing   missing   missing\n",
       "    beta_opp[2]    0.3764       NaN        NaN   missing   missing   missing\n",
       "    beta_opp[3]    0.0724       NaN        NaN   missing   missing   missing\n",
       "  next_opp_move    1.0000       NaN        NaN   missing   missing   missing\n",
       "       opp_move    2.0000       NaN        NaN   missing   missing   missing\n",
       "\n",
       "Quantiles\n",
       " \u001b[1m    parameters \u001b[0m \u001b[1m    2.5% \u001b[0m \u001b[1m   25.0% \u001b[0m \u001b[1m   50.0% \u001b[0m \u001b[1m   75.0% \u001b[0m \u001b[1m   97.5% \u001b[0m\n",
       " \u001b[90m        Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m\n",
       "\n",
       "    beta_opp[1]    0.5512    0.5512    0.5512    0.5512    0.5512\n",
       "    beta_opp[2]    0.3764    0.3764    0.3764    0.3764    0.3764\n",
       "    beta_opp[3]    0.0724    0.0724    0.0724    0.0724    0.0724\n",
       "  next_opp_move    1.0000    1.0000    1.0000    1.0000    1.0000\n",
       "       opp_move    2.0000    2.0000    2.0000    2.0000    2.0000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in depth 0\n",
      "length is 10\n",
      "next opp move is  1\n",
      "2\n",
      "move is 2\n",
      "chain is ready and it is nothing\n",
      "result is ready and it is [nothing]\n",
      "next opp is nothing\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "KeyError: key nothing not found",
     "output_type": "error",
     "traceback": [
      "KeyError: key nothing not found",
      "",
      "Stacktrace:",
      " [1] getindex at .\\dict.jl:467 [inlined]",
      " [2] best_move(::Nothing) at .\\In[26]:3",
      " [3] #39 at .\\In[50]:23 [inlined]",
      " [4] (::var\"#39#40\")(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#39#40\",(:opponent_agent, :my_history, :opponent_history, :depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(:depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(),Tuple{typeof(agent),Array{Int64,1},Array{Int64,1},Int64,Type{PG},Int64,Int64},Tuple{Int64,Type{PG},Int64,Int64}}, ::DynamicPPL.VarInfo{DynamicPPL.Metadata{Dict{DynamicPPL.VarName,Int64},Array{Distribution,1},Array{DynamicPPL.VarName,1},Array{Real,1},Array{Set{DynamicPPL.Selector},1}},Float64}, ::DynamicPPL.SampleFromPrior, ::DynamicPPL.DefaultContext, ::typeof(agent), ::Array{Int64,1}, ::Array{Int64,1}, ::Int64, ::Type{T} where T, ::Int64, ::Int64) at .\\none:0",
      " [5] macro expansion at C:\\Users\\tomer\\.julia\\packages\\DynamicPPL\\u14IH\\src\\model.jl:0 [inlined]",
      " [6] _evaluate(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#39#40\",(:opponent_agent, :my_history, :opponent_history, :depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(:depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(),Tuple{typeof(agent),Array{Int64,1},Array{Int64,1},Int64,Type{PG},Int64,Int64},Tuple{Int64,Type{PG},Int64,Int64}}, ::DynamicPPL.VarInfo{DynamicPPL.Metadata{Dict{DynamicPPL.VarName,Int64},Array{Distribution,1},Array{DynamicPPL.VarName,1},Array{Real,1},Array{Set{DynamicPPL.Selector},1}},Float64}, ::DynamicPPL.SampleFromPrior, ::DynamicPPL.DefaultContext) at C:\\Users\\tomer\\.julia\\packages\\DynamicPPL\\u14IH\\src\\model.jl:154",
      " [7] evaluate_threadunsafe(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#39#40\",(:opponent_agent, :my_history, :opponent_history, :depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(:depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(),Tuple{typeof(agent),Array{Int64,1},Array{Int64,1},Int64,Type{PG},Int64,Int64},Tuple{Int64,Type{PG},Int64,Int64}}, ::DynamicPPL.VarInfo{DynamicPPL.Metadata{Dict{DynamicPPL.VarName,Int64},Array{Distribution,1},Array{DynamicPPL.VarName,1},Array{Real,1},Array{Set{DynamicPPL.Selector},1}},Float64}, ::DynamicPPL.SampleFromPrior, ::DynamicPPL.DefaultContext) at C:\\Users\\tomer\\.julia\\packages\\DynamicPPL\\u14IH\\src\\model.jl:127",
      " [8] (::DynamicPPL.Model{var\"#39#40\",(:opponent_agent, :my_history, :opponent_history, :depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(:depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(),Tuple{typeof(agent),Array{Int64,1},Array{Int64,1},Int64,Type{PG},Int64,Int64},Tuple{Int64,Type{PG},Int64,Int64}})(::Random._GLOBAL_RNG, ::DynamicPPL.VarInfo{DynamicPPL.Metadata{Dict{DynamicPPL.VarName,Int64},Array{Distribution,1},Array{DynamicPPL.VarName,1},Array{Real,1},Array{Set{DynamicPPL.Selector},1}},Float64}, ::DynamicPPL.SampleFromPrior, ::DynamicPPL.DefaultContext) at C:\\Users\\tomer\\.julia\\packages\\DynamicPPL\\u14IH\\src\\model.jl:92",
      " [9] DynamicPPL.VarInfo(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#39#40\",(:opponent_agent, :my_history, :opponent_history, :depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(:depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(),Tuple{typeof(agent),Array{Int64,1},Array{Int64,1},Int64,Type{PG},Int64,Int64},Tuple{Int64,Type{PG},Int64,Int64}}, ::DynamicPPL.SampleFromPrior, ::DynamicPPL.DefaultContext) at C:\\Users\\tomer\\.julia\\packages\\DynamicPPL\\u14IH\\src\\varinfo.jl:126",
      " [10] VarInfo at C:\\Users\\tomer\\.julia\\packages\\DynamicPPL\\u14IH\\src\\varinfo.jl:125 [inlined]",
      " [11] step(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#39#40\",(:opponent_agent, :my_history, :opponent_history, :depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(:depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(),Tuple{typeof(agent),Array{Int64,1},Array{Int64,1},Int64,Type{PG},Int64,Int64},Tuple{Int64,Type{PG},Int64,Int64}}, ::DynamicPPL.Sampler{PG{(),AdvancedPS.ResampleWithESSThreshold{typeof(AdvancedPS.resample_systematic),Float64}}}; resume_from::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at C:\\Users\\tomer\\.julia\\packages\\DynamicPPL\\u14IH\\src\\sampler.jl:73",
      " [12] step at C:\\Users\\tomer\\.julia\\packages\\DynamicPPL\\u14IH\\src\\sampler.jl:66 [inlined]",
      " [13] macro expansion at C:\\Users\\tomer\\.julia\\packages\\AbstractMCMC\\Nw3Wn\\src\\sample.jl:78 [inlined]",
      " [14] macro expansion at C:\\Users\\tomer\\.julia\\packages\\ProgressLogging\\BBN0b\\src\\ProgressLogging.jl:328 [inlined]",
      " [15] (::AbstractMCMC.var\"#20#21\"{Bool,String,Nothing,Int64,Int64,Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},Random._GLOBAL_RNG,DynamicPPL.Model{var\"#39#40\",(:opponent_agent, :my_history, :opponent_history, :depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(:depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(),Tuple{typeof(agent),Array{Int64,1},Array{Int64,1},Int64,Type{PG},Int64,Int64},Tuple{Int64,Type{PG},Int64,Int64}},DynamicPPL.Sampler{PG{(),AdvancedPS.ResampleWithESSThreshold{typeof(AdvancedPS.resample_systematic),Float64}}},Int64,Int64})() at C:\\Users\\tomer\\.julia\\packages\\AbstractMCMC\\Nw3Wn\\src\\logging.jl:11",
      " [16] with_logstate(::Function, ::Any) at .\\logging.jl:408",
      " [17] with_logger at .\\logging.jl:514 [inlined]",
      " [18] with_progresslogger(::Function, ::Module, ::Base.CoreLogging.SimpleLogger) at C:\\Users\\tomer\\.julia\\packages\\AbstractMCMC\\Nw3Wn\\src\\logging.jl:34",
      " [19] macro expansion at C:\\Users\\tomer\\.julia\\packages\\AbstractMCMC\\Nw3Wn\\src\\logging.jl:10 [inlined]",
      " [20] mcmcsample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#39#40\",(:opponent_agent, :my_history, :opponent_history, :depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(:depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(),Tuple{typeof(agent),Array{Int64,1},Array{Int64,1},Int64,Type{PG},Int64,Int64},Tuple{Int64,Type{PG},Int64,Int64}}, ::DynamicPPL.Sampler{PG{(),AdvancedPS.ResampleWithESSThreshold{typeof(AdvancedPS.resample_systematic),Float64}}}, ::Int64; progress::Bool, progressname::String, callback::Nothing, discard_initial::Int64, thinning::Int64, chain_type::Type{T} where T, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at C:\\Users\\tomer\\.julia\\packages\\AbstractMCMC\\Nw3Wn\\src\\sample.jl:76",
      " [21] sample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#39#40\",(:opponent_agent, :my_history, :opponent_history, :depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(:depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(),Tuple{typeof(agent),Array{Int64,1},Array{Int64,1},Int64,Type{PG},Int64,Int64},Tuple{Int64,Type{PG},Int64,Int64}}, ::DynamicPPL.Sampler{PG{(),AdvancedPS.ResampleWithESSThreshold{typeof(AdvancedPS.resample_systematic),Float64}}}, ::Int64; chain_type::Type{T} where T, resume_from::Nothing, progress::Bool, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at C:\\Users\\tomer\\.julia\\packages\\Turing\\O1Pn0\\src\\inference\\Inference.jl:157",
      " [22] sample at C:\\Users\\tomer\\.julia\\packages\\Turing\\O1Pn0\\src\\inference\\Inference.jl:156 [inlined]",
      " [23] #sample#2 at C:\\Users\\tomer\\.julia\\packages\\Turing\\O1Pn0\\src\\inference\\Inference.jl:143 [inlined]",
      " [24] sample at C:\\Users\\tomer\\.julia\\packages\\Turing\\O1Pn0\\src\\inference\\Inference.jl:143 [inlined]",
      " [25] #sample#1 at C:\\Users\\tomer\\.julia\\packages\\Turing\\O1Pn0\\src\\inference\\Inference.jl:133 [inlined]",
      " [26] sample(::DynamicPPL.Model{var\"#39#40\",(:opponent_agent, :my_history, :opponent_history, :depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(:depth, :discrete_sampler, :discrete_sampler_hyper_param, :num_of_iterations),(),Tuple{typeof(agent),Array{Int64,1},Array{Int64,1},Int64,Type{PG},Int64,Int64},Tuple{Int64,Type{PG},Int64,Int64}}, ::PG{(),AdvancedPS.ResampleWithESSThreshold{typeof(AdvancedPS.resample_systematic),Float64}}, ::Int64) at C:\\Users\\tomer\\.julia\\packages\\Turing\\O1Pn0\\src\\inference\\Inference.jl:133",
      " [27] top-level scope at In[51]:2",
      " [28] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091"
     ]
    }
   ],
   "source": [
    "m = agent(agent, ones(Int64, 10) , 2* ones(Int64, 10))\n",
    "chain = sample(m, PG(3), 3)\n",
    "display(chain)\n",
    "return_values = generated_quantities(m, chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple aux function that return the most common sample - majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "most_common (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function most_common(samples)\n",
    "    count = Dict(1 => 0, 2 => 0, 3 => 0)\n",
    "    for i in 1:length(samples)\n",
    "        count[samples[i]] += 1\n",
    "    end\n",
    "    max_k, max_v = -1 , -1\n",
    "    for (k, v) in count\n",
    "        if v > max_v\n",
    "            max_k , max_v = k, v\n",
    "        end\n",
    "    end\n",
    "    return max_k\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This method is used for the simulation, each model make a move at the end of this function, as a result of reasoning of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "move (generic function with 2 methods)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function move(agent, other_agent, my_history, other_agent_history, my_depth=1)\n",
    "    other_agent_history = length(other_agent_history) > 0 ? other_agent_history : [1]\n",
    "    my_history = length(my_history) > 0 ? my_history : [1]\n",
    "    my_history = Array{Int}(my_history)\n",
    "    other_agent_history = Array{Int}(other_agent_history)\n",
    "    chain = sample(agent(other_agent, my_history, other_agent_history, my_depth), PG(5), 5, progress = true)\n",
    "    return most_common(chain[:\"next_move\"])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move(agent, agent, [1, 2], [2 , 3])\n",
    "move(agent, agent, [1, 2, 3], [2 , 3, 2])\n",
    "move(agent, agent, [1, 2, 2, 1], [2 , 3, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game simulation , given two agent models , depth params, let them play num of simulation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "game (generic function with 3 methods)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function game(first_player_depth = 1, second_player_depth = 1)\n",
    "    first_player = agent\n",
    "    second_player = agent\n",
    "    num_of_simulations = 100\n",
    "    first_player_history = []\n",
    "    second_player_history = []\n",
    "    for i in 1:num_of_simulations\n",
    "        m1 = move(first_player, second_player, first_player_history, second_player_history, first_player_depth)\n",
    "#         println(\"player1 choose $m1\")\n",
    "        push!(first_player_history, m1)\n",
    "        m2 = move(second_player, first_player, second_player_history, first_player_history, second_player_depth)\n",
    "#         println(\"player2 choose $m2\")\n",
    "        push!(second_player_history, m2)\n",
    "#         println(\"in simulation $i first player chose $m1 second player chose $m2\")\n",
    "    end\n",
    "    return first_player_history, second_player_history\n",
    " end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score (generic function with 1 method)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function score(history)\n",
    "    first_player_history, second_player_history = history\n",
    "    first_wins = 0\n",
    "    ties = 0\n",
    "    second_wins = 0\n",
    "    wins = Dict(1 => 3, 2 => 1, 3 => 2)\n",
    "    for i in 1:length(first_player_history)\n",
    "        if wins[first_player_history[i]] == second_player_history[i]\n",
    "            first_wins += 1\n",
    "        elseif wins[second_player_history[i]] == first_player_history[i]\n",
    "            second_wins += 1\n",
    "        else\n",
    "            ties += 1\n",
    "        end\n",
    "    end\n",
    "    return first_wins, ties, second_wins\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "display_score (generic function with 1 method)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function display_score(score)\n",
    "    num_of_wins_first, num_of_ties, num_of_wins_second = score\n",
    "    println(\"first player won: $num_of_wins_first\")\n",
    "    println(\"second player won: $num_of_wins_second\") \n",
    "    println(\"ties: $num_of_ties\") \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both player plays without theory of mind - equivilent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first player won: 14\n",
      "second player won: 11\n",
      "ties: 75\n"
     ]
    }
   ],
   "source": [
    "display_score(score(game(0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd player plays with one level of theory of mind -> 2nd wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first player won: 16\n",
      "second player won: 28\n",
      "ties: 56\n"
     ]
    }
   ],
   "source": [
    "display_score(score(game(0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st player plays with one level of theory of mind -> 1st wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first player won: 32\n",
      "second player won: 7\n",
      "ties: 61\n"
     ]
    }
   ],
   "source": [
    "display_score(score(game(1, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both players play with one level of theory of mind -> preety much equivelent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first player won: 24\n",
      "second player won: 23\n",
      "ties: 53\n"
     ]
    }
   ],
   "source": [
    "display_score(score(game(1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
