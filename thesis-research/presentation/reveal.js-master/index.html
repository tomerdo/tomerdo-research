<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h2>Reasoning About Future With Probabilistic Programs</h2>
					<p>Thesis defence</p>
					<p>Tomer Dobkin under the supervision of Dr. David Tolpin</p>
				</section>
				<section>
					<ul>
						<li>Background</li>
						<li>The Fable of Bob and Alice</li>
						<li>Determenistic Preferences</li>
						<li>Stochastic Preferences</li>
						<li>Reinformencement Learning as Infernce</li>
						<li>Related Work</li>
						<li>Discussion</li>
					</ul>
				</section>
				<section>
					<section>
						Background
					</section>
					<section>Probabilistic Programming</section>
					<section>
						<h3>Probabilistic Program Example</h3>
						<img src="assets/figures/coin-psaudo.png"></img>
						<aside class="notes">
							We have here a simple example of probablistic program.
							running forward - this sampling points of y
							running infernce (backward) over observation - we compute the posterior of theta
  						</aside>
					</section>
					<section>
					  <h3>Probabilistic Program Example</h3>
						  <img src="assets/figures/survey-psaudo.png"></img>
							<aside class="notes">
								We have here a more complex example of probablistic program.
								Condecut a survey of how much are satifies with his salery,
								without knowing each indivdual answers.
								We tell them:
								1. Toss a coin, if it is true:
								2. Answer the your true opinoin
								3. If false - toss again - and anser true iff Head
	  						</aside>
					</section>
					<section>
							<img src="assets/figures/posterior-update-animation.gif"></img>
					</section>
					<section>
							<img src="assets/figures/bernoulli-update.gif"></img>
					</section>
					<section>
						Planning as Inference
					</section>
						<section>
							<p>policy: $\pi_\theta(E)$</p>
							<p class="fragment fade-in">latent variables: $\theta$, environment: $E$</p>
							<p class="fragment fade-in">reward: $r \sim \pi_\theta(E)$</p>
							<p class="fragment fade-in">The posterior distribution of policy parameters: $p(\theta\vert E)$</p>
							<p class="fragment fade-in">$p(\theta\vert E) \propto p(\theta)\exp\left(\mathbb{E}\left[r \sim \pi_\theta(E)\right]\right)$</p>
							<p class="fragment fade-in">Posterior inference on the above gives a distribution of policy parameters</p>
							<aside class="notes">
								Planning, as a discipline of artificial intelligence, considers
								agents acting in environments. The agents have
								beliefs about their environments and perform actions which bring
								them rewards (or regrets). AI planning is concerned with
								algorithms that search for policies --- mappings from
								agents' beliefs about the environment to their actions. In
								planning-as-inference approach, policy search
								is expressed as inference in an appropriate probabilistic model.
								A probabilistic program reifying the planning-as-inference
								approach encodes instantiation of policy $\pi_\theta(E)$,
								depending on latent parameters $\theta$, in environment $E$. In
								the course of execution, the program computes the reward $r \sim
								\pi_\theta(E)$, stochastic in general.  The posterior
								distribution of policy parameters $p(\theta\vert E)$, conditioned on
								the environment, is defined in terms of their prior distribution
								$p(\theta)$ and of the expected reward:
								Posterior inference on \eqref{eqn:planning-as-inference} gives a
								distribution of policy parameters, with the mode of the
								distribution corresponding to the policy maximizing the expected
								reward.
							</aside>
						</section>
					<section>
						Stochastic Conditioning
					</section>
					<section>
						<p>$p(x\vert y=y_0)$</p>
						<p class="fragment fade-in">$p(x\vert y \sim D_0)$</p>
						<p class="fragment fade-in">$(p(x, y), D)$</p>
						<p class="fragment fade-in">The objective is to infer $p(x\vert y \sim D)$ the distribution of $x$ given $D$</p>
						<p class="fragment fade-in">rather than an individual observation $y$</p>
						<img src="assets/figures/stochastic-notation.png" class="fragment fade-in"></img>

						<aside class="notes">
							Stochastic conditioning extends
							deterministic conditioning $p(x\vert y=y_0)$,
							i.e.~conditioning on some random variable in our program $y$
							taking on a particular value $y_0$, to conditioning $p(x\vert y
							\sim D_0)$ on $y$ having the marginal distribution $D_0$.
							A probabilistic model with stochastic conditioning is a tuple
							$(p(x, y), D)$ where
							\begin{itemize}
							    \item $p(x, y)$ is the joint probability density of random
							        variable $x$ and observation $y$,
							    \item $D$  is the distribution from which observation $y$ is
							        marginally sampled, and it has a density $q(y)$.
							\end{itemize}

							Unlike in the usual setting, the objective is to infer $p(x\vert y
							\sim D)$, the distribution of $x$ given \textit{distribution}
							$D$, rather than an individual observation $y$.  To accomplish
							this objective, one must be able to compute $p(x, y \sim
							D)$, a possibly unnormalized density on $x$ and distribution
							$D$.
							By convention, in statistical notation $y \sim D$ is placed
							above a rule to denote that distribution $D$ is observed through
							$y$ and is otherwise unknown to the model
						</aside>
					</section>
					<section>
						Epistemtic Reasoning
					</section>
					<section>
						<p>Epistemic reasoning, also known as theory of mind, is mutual reasoning of multiple agents about each others' beliefs and intentions.</p>
						<p class="fragment fade-in">Each agent's policy is mutually conditioned on other agents' policies</p>
						<p class="fragment fade-in">Probabilistic programming allows natural representation of epistemic reasoning</p>
						<p class="fragment fade-in">Inference in multiagent settings requires nested conditioning</p>
						<p class="fragment fade-in">The recursion can unroll indefinitely -> recursion depth is bounded by a constant.</p>
						<aside class="notes">
							Epistemic reasoning, also known as theory of
							mind, is mutual reasoning of multiple agents about each others'
							beliefs and intentions. Epistemic reasoning comes up in
							the planning-as-inference paradigm when each agent's policy is
							mutually conditioned on other agents' policies.
							Probabilistic programming allows natural representation of
							epistemic reasoning: the program modeling an agent invokes
							inference on the programs modeling the rest of the agents.
							However, this means that inference in multiagent settings
							requires nested conditioning, which is computationally
							challenging in general, although attempts are being made to find
							efficient implementations of nested conditioning in certain
							settings.  Since each agent's model recursively
							refers to models of other agents, the recursion can unroll
							indefinitely.  In a basic approach, the
							recursion depth is bounded by a constant.
						</aside>
					</section>
				</section>
				<section>
					<section>
						The Fable of Bob and Alice
						<aside class="notes">
							The use case we will explore in the research
						</aside>
					</section>
					<section>
						<blockquote>
							Bob and Alice want to meet in a bar, but Bob left his phone
							at home.<br>
							There are two bars, which Bob and Alice visit with different frequencies.<br>
							Which bar Bob and Alice should head if they want to meet?
						</blockquote>
						<aside class="notes">
							classic example from game of theory research,
							introduced in the context of probablistic programming
							first in: Modeling Agents with Probabilistic Programs.
							We will talk about it later.
  					 </aside>
					</section>
					<section>
						<img src="assets/figures/bob-alice-intro-1.png"></img>
						<aside class="notes">
							Let's try to model it abstractly
  					 </aside>
					</section>
					<section>
						<img src="assets/figures/bob-alice-intro-epsiode-infer.png"></img>
						<aside class="notes">
							Here we model one episode unrolling
  					 </aside>
					</section>
				</section>
				<section>
					<section>
						Determenistic Preferences
					</section>
					<section>
						The fable of Bob and Alice is underspecified
						<aside class="notes">
							We know how strong Bob
							or Alice prefer the first bar over the second one, or vice versa, however,
							we do not know how strong Bob and Alice prefer to meet (or avoid) each
							other.
							We think that the study case is not specified enough,
							what is the meaning of this proablities, if in the end
							the most important mission is that they will meet,
							ofcourse if we talking about probablity greater 0.5
							for both of them for the same bar this is the rational thing to do
  					 </aside>
					</section>
					<section>
						 <ol>
							 <li>$r_1$ - the reward for visiting the first bar;</li>
							 <li>$r_2$ - the reward for visiting the second bar;</li>
							 <li>$r_m$ - the reward for meeting the other person.</li>
						 </ol>
						 <aside class="notes">
 							To add specification and formalism we introduce 3 rewards
							for each one of them. With this we can quantify their "intersts"
   					 </aside>
					</section>
					<section>
						<p class="fragment fade-in">$r_{Alice}^1 > r_{Alice}^2$</p>
						<p class="fragment fade-in">$r_{Alice}^1 + \hat p_{Bob}^1r_{Alice}^m >
						r_{Alice}^2 + (1 - \hat p_{Bob}^1)r_{Alice}^m$</p>

						<aside class="notes">
							Alice prefers the first bar ($r_{Alice}^1 > r_{Alice}^2$) and is
							rational, she should always go to the first bar.
							If, in addition, Alice ascribes reward $r_{Alice}^m$ to meeting Bob,
							and believes that Bob goes to the first bar with
							probability $\hat p_{Bob}^1$, then she should go to the first
							bar if $r_{Alice}^1 + \hat p_{Bob}^1r_{Alice}^m >
							r_{Alice}^2 + (1 - \hat p_{Bob}^1)r_{Alice}^m$, and to
							the second bar otherwise,
						</aside>
					</section>
					<section>
						<p class="fragment fade-in">$r_{Alice}^1 = r_{Alice}^2 +
							 \log\left(\frac {p_{Alice}^1} {1 - p_{Alice}^1}\right)$</p>
						<p class="fragment fade-in">$p_{Alice}^1 = 0.55$</p>
						<p class="fragment fade-in">$r_1 \approx r_2 + 0.2$</p>
					<aside class="notes">
						$r_{Alice}^1$ and $r_{Alice}^2$ such that $r_{Alice}^1 =
						r_{Alice}^2 + \log\left(\frac {p_{Alice}^1} {1 -
						p_{Alice}^1}\right)$. Concretely, for $p_{Alice}^1 = 0.55$ we
						obtain $r_1 \approx r_2 + 0.2$; only the difference between
						rewards matters in a softmax agent, rather than their absolute
						values.
						The softmax agent was proposed and is broadly used for
						modeling `approximately optimal agents`.
						However, stochastic behavior is not unusual, and we should be
						able to model it. To reconcile rewards and stochastic action
						choice, the softmax agent was proposed and is broadly used for
						modeling `approximately optimal agents'.
						A softmax agent
						chooses actions stochastically, with probabilities proportional
						to the exponentiated action utilities.
					</aside>
					</section>
						<section>
							<p class="fragment fade-in">$u_{Alice}^1 = r_{Alice}^1 + \hat p_{Bob}^1r_{Alice}^m$</p>
							<p class="fragment fade-in">$u_{Alice}^2 = r_{Alice}^2 + (1 - \hat p_{Bob}^1)r_{Alice}^m$</p>
							<p class="fragment fade-in">$\Pr(\mathbb{I}_{Alice}^1) = \frac {\exp(u_{Alice}^1)} {\exp(u_{Alice}^1) + \exp(u_{Alice}^2)}$</p>
							<!-- img src="assets/figures/determinisic-1.png"></img -->
						</section>
				</section>
				<section>
					<section>
					Stochastic Preferences
					</section>
					<section>
						<img src="assets/figures/stochastic-1.png"></img>
						<aside class="notes">
							The model, in a superficially confusing way, generates Bob’s location I1
							Bob
							based on Alice’s desire to meet Bob. However, the confusion can be resolved
							by the following interpretation: when Alice chooses to go to the first bar, it
							is because, in addition to the slight preference over the second bar (line 1),
							she believes that Bob will also go to the first bar with probability pmA
							lice. It
							is Alice’s anticipations of Bob’s behavior that are generated by the model,
							rather than events.
						</aside>
					</section>
					<section>
						<img src="assets/figures/stochastic-2.png"></img>
						<aside class="notes">
							Since Model 3 generates anticipations, it should also be conditioned on
							anticipations. Let us, at this point, accept as a fact that Alice concludes that
							Bob goes to the first bar with probability ˆ p1
							Bob. Then, the model must be
							conditioned on I1
							Bob being distributed as Bernoulli( ˆ p1
							Bob). This involves an
							application of stochastic conditioning
						</aside>
					</section>
					<section>
							<img src="assets/figures/stochastic-3.png"></img>
					</section>
					<section>
						Planning as inference with probabilistic preferences can be encoded as a Bayesian
						generative model using probabilities only, without resort to
						rewards.
						<aside class="notes">
							A direct corollary from (5.1) and (5.2) is that Planning as inference with probabilistic preferences
						</aside>
					</section>
					<section>
						<blockquote>
							Preferences are naturally probabilistic in general and should be
							expressed as probabilities of choices under clairvoyance.<br><br>
							`Deterministic` specification of preferences through rewards is
							an indirect way to encode probabilistic preferences.
						</blockquote>
						<aside class="notes">
							Reward maximization algorithms for policy search are in fact
							maximum a posteriori approximations of the
							distributions of optimal behaviors, which are inherently
							stochastic.
						</aside>
					</section>
				</section>
				<section>
					<section>
						Reinformencement Learning as Infernce
						<aside class="notes">
							After we have seen a formalization let's try to connect it to the bigger picutre
						</aside>
					</section>
					<section>
							<img src="assets/figures/mdp-1.png"></img>
					</section>
					<section>
							<img src="assets/figures/mdp-2.png"></img>
					</section>
					<section>
							<img src="assets/figures/mdp-3.png"></img>
							<aside class="notes">
									D_a - prior distribution over actions
									D_b - agent's belief about the distributiion of other agent's actions in state given that action a_a is taken
									D_s - distribtuion of states from state s that agent a desired to pass
							</aside>
					</section>
					<section>
							<img src="assets/figures/mdp-am.png"></img>
							<aside class="notes">
								Apprenticeship learning [AN04] is concerned with a situation in which
								the agent preferences are not given explicitly, however, there are observed
								MDP trajectories, from which the preferences can be inferred.
							</aside>
					</section>
				</section>
				<section>
					<section>
						Empirical Evaluation
					</section>
					<section>
						<h2>Bob and Alice Want to Meet - perfer same bar</h2>
						<img src="assets/figures/co-operative-1.png"></img>
						<aside class="notes">
							The original case, both players want to meet, we can see that it start from prior
							and rises to 0.83 and sty there (when p is 0.9), prefer the same bar
						</aside>
					</section>
					<section>
						<h2>Bob and Alice Want to Meet - prefer different bars</h2>
						<img src="assets/figures/different-bars-1.png"></img>
					</section>
					<section>
						<h2>Bob chases Alice, Alice Avoids Bob</h2>
						<div class="r-stack">
							<img src="assets/figures/bob-chases-1.png" class="fragment fade-in-then-out"></img>
							<img src="assets/figures/bob-chases-2.png" class="fragment fade-in-then-out"></img>
						</div>
					</section>
					<section>
						<h2>Bob and Alice Avoid Each Other</h2>
						<div class="r-stack">
							<img src="assets/figures/both-avoid-1.png" class="fragment fade-in-then-out"></img>
							<img src="assets/figures/both-avoid-2.png" class="fragment fade-in-then-out"></img>
						</div>
					</section>
					<section>
						<h2>Alice learns Bob's preferences</h2>
						<img src="assets/figures/bob-alice-learned.png"></img>
					</section>
					<section>
						The Sailing Problem
					</section>
					<section>
							<img src="assets/figures/sailing-fig.png"></img>
							<aside class="notes">
								The sailing problem (Figure 7.8) is a popular benchmark problem for search
								and planning. A sailing boat must travel between the opposite corners A
								and B of a square lake of a given size. At each step, the boat can head in 8
								directions (legs) to adjacent squares (Figure 7.8a). The unit distance cost of
								movement depends on the wind (Figure 7.8b), which can also blow in 8
								directions. There are five relative boat and wind directions and associated
								costs: into, up, cross, down, and away. The cost of sailing into the wind is
								prohibitively high, upwind is the highest feasible, and away from the wind
								is the lowest. The side of the boat off which the sail is hanging is called the
							</aside>

					</section>
					<section>
							<img src="assets/figures/sailing-params.png"></img>
							<aside class="notes">
							For any given lake size, there is a non-parametric stochastic policy that
							tabulates the distribution of legs for each combination of location, tack,
							and wind. However, such policy does not generalize well — if the lake
							area increases, due to a particularly rainy year for example, the policy is
							not applicable to the new parts of the lake. In this case study, we infer
							instead a generalizable parametric policy balancing between hesitation in
							anticipation for a better wind and rushing to the goal at any cost. The
							policy chooses a leg with the log-probability equal to the euclidian distance
							between the position after the leg and the goal, multiplied by the policy
							parameter q (the leg directed into the wind is excluded from choices). The
							greater the q, the higher is the probability that a leg bringing the boat closer
							to the goal will be chosen:
						</aside>
					</section>
					<section>
							<img src="assets/figures/sailing-model.png"></img>
							<aside class="notes">
								The first agent, Alice, is the boat. The agent chooses a path across the
							lake.
							• The second agent, Bob, is the wind. The agent chooses a random walk
							of wind direction along the boat’s path. The wind is a neutral agent—
							it does not try to either help or tamper with the boat.
							• The boat’s path, stochastically conditioned on the wind, has the prob
						</aside>
					</section>
					<section>
							<img src="assets/figures/sailing-results.png"></img>
						<aside class="notes">
								The model parameters (cost and wind change probabilities), same as
								in [KS06, TS12], are shown in Table 7.1. A non-informative improper prior
								is imposed on q. We fit the model using pseudo-marginal Metropolis-
								Hastings [AR09] and used 10 000 samples to approximate the posterior.
								Figure 7.9 shows the posterior distribution of the unit cost. Table 7.2 shows
								the expected travel costs, with the expectations estimated both over the unit
								cost and the wind. The inferred travel costs are compared to the travel costs
								of the ‘optimal’ policy and of the greedy policy, according to which the
								boat always heads in the direction of the steepest decrease of the distance
								to the goal. One can see that the inferred policy attains an expected travel
								cost lying between the greedy policy and the ‘optimal’ policy, as one would
								anticipate.
					</aside>
					</section>
					<section>
							<img src="assets/figures/sailing-results-2.png"></img>
					</section>
				</section>
				<section>
					<section>
						Related Work
						<aside class="notes">

						</aside>
					</section>
					<section>
						<h2>agentmodels</h2>
						<img src="assets/figures/agent-models-1.png"></img>
						<aside class="notes">
							Alice’s deliberation is
							irrational. If Alice wants to meet Bob and knows that Bob chooses the first
							bar more often, no matter to which extent, she must always choose the first
							bar! Moreover, the model’s recommendation to choose the first bar with
							the probability that she meets Bob in the first bar if she does not choose the
							first bar more often than usual sounds at least surprising and suggests that
							the model is wrong
						</aside>
					</section>
					<section>
						<h2>Nested Reasoning About Autonomous Agents Using Probabilistic Programs</h2>
						<img src="assets/figures/seaman.png"></img>
						<aside class="notes">
							Approach a pursuit-evasion problem, in the form of
							a chaser (sheriff) and a runner (thief) moving through city
							streets, with tools of probabilistic programming and planning as
							inference. A simpler but similar setting can be expressed using
							Bob and Alice with adversarial preferences with respect to
							meeting each other: Bob (the chaser) wants to meet Alice, but
							Alice (the runner) wants to avoid Bob.
							formulates the inference problem in terms of the
							Boltzmann distribution of trajectories based on the travel time
							of the runner, as well as the time the runner seen by the
							chaser, positive for the chaser (wants to see the runner as long
							as possible) and negative for the runner (wants to avoid being
							seen by the chaser as much as possible).  However, looking
							for efficient inference, this work proposes to replace nested
							conditioning by conditioning of each level of reasoning on a
							single sample from the previous level. This is, again,
							conditioning on the future as though the future were known, and
							leads to wrong results.

							The problem can be hard to realize on the complicated setting
							explored in the work, but becomes obvious on the example of Bob
							and Alice. Consider, for simplicity, the setting in which Alice
							has equal preferences regarding the bars and attributes reward 1
							to avoiding Bob and 0 to meeting him. Bob chooses the first bar
							in 55\% of cases; Alice employs a single level of epistemic
							reasoning. It is easy to see that the optimal policy for Alice
							is to always go to the second bar for the expected reward or 0.55.
							However, the mode of the posterior inferred using the algorithm
							in~\cite{SMW18} is for Alice to choose the second bar with
							probability of 0.55, for the expected reward of 1! This is
							because Alice's decision is (erroneously) conditioned on Bob's
							anticipated choice of a bar, and hence Alice pretends that she
							can always choose the other bar (which she cannot).

						</aside>
					</section>
				</section>
				<section>
					<section>
							Discussion
					</section>
					<section>
						<ul>
							<li  class="fragment fade-in">The agent’s reasoning about preferred future behavior, can be formulated as Bayesian inference</li>
							<li class="fragment fade-in">Probability distributions can be used to specify all aspects of uncertainty or
							stochasticity, including agent preferences</li>
							<li class="fragment fade-in">Rewards can be interpreted as log-odds of stochastic preferences and do not need to be explicitly introduced.</li>
							<li class="fragment fade-in">Planning algorithms maximizing the expected utility find maximum a posteriori characterization of the rational policy distribution.</li>
						</ul>
						<aside class="notes">
							We demonstrated that reinforcement learning, that is, the agent’s reasoning
							about preferred future behavior, can be formulated as Bayesian inference.
							Probability distributions can be used to specify all aspects of uncertainty or
							stochasticity, including agent preferences. Rewards can be interpreted as
							log-odds of stochastic preferences and do not need to be explicitly introduced.
							Planning algorithms maximizing the expected utility find maximum
							a posteriori characterization of the rational policy distribution.
						</aside>
					</section>
			</section>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});
		</script>
	</body>
</html>
