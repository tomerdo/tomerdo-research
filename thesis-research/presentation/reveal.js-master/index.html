<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h2>Reasoning About Future With Probabilistic Programs</h2>
					<p>Thesis defence</p>
					<p>Tomer Dobkin under the supervision of Dr. David Tolpin</p>
				</section>
				<section>
					<ul>
						<li>Probabilistic Programming</li>
						<li>The Fable of Bob and Alice</li>
						<li>Determenistic Prefernces</li>
						<li>Stochastic Prefernces</li>
						<li>Reinformencement Learning as Infernce</li>
						<li>Related Work</li>
						<li>Discussion</li>
					</ul>
				</section>
				<section>
					<section>Probabilistic Programming</section>
					<section>
						<h3>Probabilistic Program Example</h3>
						<img src="assets/figures/coin-psaudo.png"></img>
						<aside class="notes">
							We have here a simple example of probablistic program.
							running forward - this sampling points of y
							running infernce (backward) over observation - we compute the posterior of theta
  						</aside>
					</section>
					<section>
					  <h3>Probabilistic Program Example</h3>
						  <img src="assets/figures/survey-psaudo.png"></img>
							<aside class="notes">
								We have here a more complex example of probablistic program.
								Condecut a survey of how much are satifies with his salery,
								without knowing each indivdual answers.
								We tell them:
								1. Toss a coin, if it is true:
								2. Answer the your true opinoin
								3. If false - toss again - and anser true iff Head
	  						</aside>
					</section>
				</section>
				<section>
					<section>
						The Fable of Bob and Alice
					</section>
					<section>
						<blockquote>
							Bob and Alice want to meet in a bar, but Bob left his phone
							at home. There are two bars, which Bob and Alice visit with
							different frequencies. Which bar Bob and Alice should head if
							they want to meet?
						</blockquote>
						<aside class="notes">
							We have here a simple example of probablistic program.
							running forward - this sampling points of y
							running infernce (backward) over observation - we compute the posterior of theta
  					 </aside>
					</section>
					<section>
						<img src="assets/figures/bob-alice-intro-1.png"></img>
						<aside class="notes">
							We have here a simple example of probablistic program.
							running forward - this sampling points of y
							running infernce (backward) over observation - we compute the posterior of theta
  					 </aside>
					</section>
					<section>
						<img src="assets/figures/bob-alice-intro-epsiode-infer.png"></img>
						<aside class="notes">
							We have here a simple example of probablistic program.
							running forward - this sampling points of y
							running infernce (backward) over observation - we compute the posterior of theta
  					 </aside>
					</section>
				</section>
				<section>
					<section>
						Determenistic Prefernces
					</section>
					<section>
						The fable of Bob and Alice is underspecified: we know how strong Bob
						or Alice prefer the first bar over the second one, or vice versa, however,
						we do not know how strong Bob and Alice prefer to meet (or avoid) each
						other.
					</section>
					<section>
						 <ol>
							 <li>$r_1$ - the reward for visiting the first bar;</li>
							 <li>$r_2$ - the reward for visiting the second bar;</li>
							 <li>$r_m$ - the reward for meeting the other person.</li>
						 </ol>
					</section>
					<section>
						Alice prefers the first bar ($r_{Alice}^1 > r_{Alice}^2$) and is
						rational, she should always go to the first bar.
						If, in addition, Alice ascribes reward $r_{Alice}^m$ to meeting Bob,
						and believes that Bob goes to the first bar with
						probability $\hat p_{Bob}^1$, then she should go to the first
						bar if $r_{Alice}^1 + \hat p_{Bob}^1r_{Alice}^m >
						r_{Alice}^2 + (1 - \hat p_{Bob}^1)r_{Alice}^m$, and to
						the second bar otherwise,
					</section>
					<section>
						The softmax agent was proposed and is broadly used for
						modeling `approximately optimal agents`.
						<aside class="notes">
							However, stochastic behavior is not unusual, and we should be
							able to model it. To reconcile rewards and stochastic action
							choice, the softmax agent was proposed and is broadly used for
							modeling `approximately optimal agents'.
							A softmax agent
							chooses actions stochastically, with probabilities proportional
							to the exponentiated action utilities.
  						</aside>
					</section>
					<section>
						$r_{Alice}^1$ and $r_{Alice}^2$ such that $r_{Alice}^1 =
						r_{Alice}^2 + \log\left(\frac {p_{Alice}^1} {1 -
						p_{Alice}^1}\right)$. Concretely, for $p_{Alice}^1 = 0.55$ we
						obtain $r_1 \approx r_2 + 0.2$; only the difference between
						rewards matters in a softmax agent, rather than their absolute
						values.
					</section>
						<section>
							<img src="assets/figures/determinisic-1.png"></img>
						</section>
				</section>
				<section>
					<section>
					Stochastic Prefernces
					</section>
					<section>
						<img src="assets/figures/stochastic-1.png"></img>
						<aside class="notes">
							The model, in a superficially confusing way, generates Bob’s location I1
							Bob
							based on Alice’s desire to meet Bob. However, the confusion can be resolved
							by the following interpretation: when Alice chooses to go to the first bar, it
							is because, in addition to the slight preference over the second bar (line 1),
							she believes that Bob will also go to the first bar with probability pmA
							lice. It
							is Alice’s anticipations of Bob’s behavior that are generated by the model,
							rather than events.
						</aside>
					</section>
					<section>
						<img src="assets/figures/stochastic-2.png"></img>
						<aside class="notes">
							Since Model 3 generates anticipations, it should also be conditioned on
							anticipations. Let us, at this point, accept as a fact that Alice concludes that
							Bob goes to the first bar with probability ˆ p1
							Bob. Then, the model must be
							conditioned on I1
							Bob being distributed as Bernoulli( ˆ p1
							Bob). This involves an
							application of stochastic conditioning
						</aside>
					</section>
					<section>
							<img src="assets/figures/stochastic-3.png"></img>
					</section>
					<section>
						A direct corollary from (5.1) and (5.2) is that planning as inference
						with probabilistic preferences can be encoded as a Bayesian
						generative model using probabilities only, without resort to
						rewards.
					</section>
					<section>
						<blockquote>
							Preferences are naturally probabilistic in general and should be
							expressed as probabilities of choices under clairvoyance.
							`Deterministic` specification of preferences through rewards is
							an indirect way to encode probabilistic preferences. Reward
							maximization algorithms for policy search are in fact
							maximum a posteriori approximations of the
							distributions of optimal behaviors, which are inherently
							stochastic.
						</blockquote>
					</section>
				</section>
				<section>
					<section>
						Reinformencement Learning as Infernce
					</section>
					<section>
							<img src="assets/figures/mdp-1.png"></img>
					</section>
					<section>
							<img src="assets/figures/mdp-2.png"></img>
					</section>
					<section>
							<img src="assets/figures/mdp-3.png"></img>
							<aside class="notes">
									D_a - prior distribution over actions
									D_b - agent's belief about the distributiion of other agent's actions in state given that action a_a is taken
									D_s - distribtuion of states from state s that agent a desired to pass
							</aside>
					</section>
					<section>
							<img src="assets/figures/mdp-am.png"></img>
							<aside class="notes">
								Apprenticeship learning [AN04] is concerned with a situation in which
								the agent preferences are not given explicitly, however, there are observed
								MDP trajectories, from which the preferences can be inferred.
							</aside>
					</section>
				</section>
				<section>
					<section>
						Empirical Evaluation
					</section>
					<section>
						<h2>Bob and Alice Want to Meet - perfer same bar</h2>
						<img src="assets/figures/co-operative-1.png"></img>
						<aside class="notes">
							The original case, both players want to meet, we can see that it start from prior
							and rises to 0.83 and sty there (when p is 0.9), prefer the same bar
						</aside>
					</section>
					<section>
						<h2>Bob and Alice Want to Meet - prefer different bars</h2>
						<img src="assets/figures/different-bars-1.png"></img>
					</section>
					<section>
						<h2>Bob chases Alice, Alice Avoids Bob</h2>
						<div class="r-stack">
							<img src="assets/figures/bob-chases-1.png" class="fragment fade-in-then-out"></img>
							<img src="assets/figures/bob-chases-2.png" class="fragment fade-in-then-out"></img>
						</div>
					</section>
					<section>
						<h2>Bob and Alice Avoid Each Other</h2>
						<div class="r-stack">
							<img src="assets/figures/both-avoid-1.png" class="fragment fade-in-then-out"></img>
							<img src="assets/figures/both-avoid-2.png" class="fragment fade-in-then-out"></img>
						</div>
					</section>
					<section>
						<h2>Alice learns Bob's Prefernces</h2>
						<img src="assets/figures/bob-alice-learned.png"></img>
					</section>
					<section>
						The Sailing Problem
					</section>
					<section>
							<img src="assets/figures/sailing-fig.png"></img>
							<aside class="notes">
								The sailing problem (Figure 7.8) is a popular benchmark problem for search
								and planning. A sailing boat must travel between the opposite corners A
								and B of a square lake of a given size. At each step, the boat can head in 8
								directions (legs) to adjacent squares (Figure 7.8a). The unit distance cost of
								movement depends on the wind (Figure 7.8b), which can also blow in 8
								directions. There are five relative boat and wind directions and associated
								costs: into, up, cross, down, and away. The cost of sailing into the wind is
								prohibitively high, upwind is the highest feasible, and away from the wind
								is the lowest. The side of the boat off which the sail is hanging is called the
							</aside>

					</section>
					<section>
							<img src="assets/figures/sailing-params.png"></img>
							<aside class="notes">
							For any given lake size, there is a non-parametric stochastic policy that
							tabulates the distribution of legs for each combination of location, tack,
							and wind. However, such policy does not generalize well — if the lake
							area increases, due to a particularly rainy year for example, the policy is
							not applicable to the new parts of the lake. In this case study, we infer
							instead a generalizable parametric policy balancing between hesitation in
							anticipation for a better wind and rushing to the goal at any cost. The
							policy chooses a leg with the log-probability equal to the euclidian distance
							between the position after the leg and the goal, multiplied by the policy
							parameter q (the leg directed into the wind is excluded from choices). The
							greater the q, the higher is the probability that a leg bringing the boat closer
							to the goal will be chosen:
						</aside>
					</section>
					<section>
							<img src="assets/figures/sailing-model.png"></img>
							<aside class="notes">
								The first agent, Alice, is the boat. The agent chooses a path across the
							lake.
							• The second agent, Bob, is the wind. The agent chooses a random walk
							of wind direction along the boat’s path. The wind is a neutral agent—
							it does not try to either help or tamper with the boat.
							• The boat’s path, stochastically conditioned on the wind, has the prob
						</aside>
					</section>
					<section>
							<img src="assets/figures/sailing-results.png"></img>
						</aside class="notes">
								The model parameters (cost and wind change probabilities), same as
								in [KS06, TS12], are shown in Table 7.1. A non-informative improper prior
								is imposed on q. We fit the model using pseudo-marginal Metropolis-
								Hastings [AR09] and used 10 000 samples to approximate the posterior.
								Figure 7.9 shows the posterior distribution of the unit cost. Table 7.2 shows
								the expected travel costs, with the expectations estimated both over the unit
								cost and the wind. The inferred travel costs are compared to the travel costs
								of the ‘optimal’ policy and of the greedy policy, according to which the
								boat always heads in the direction of the steepest decrease of the distance
								to the goal. One can see that the inferred policy attains an expected travel
								cost lying between the greedy policy and the ‘optimal’ policy, as one would
								anticipate.
					</aside>
					</section>
					<section>
							<img src="assets/figures/sailing-results-2.png"></img>
					</section>
				</section>
				<section>
					<section>
						Related Work
					</section>
					<section>
						<h2>agentmodels</h2>
						<img src="assets/figures/agent-models-1.png"></img>
						<aside class="notes">
							Alice’s deliberation is
							irrational. If Alice wants to meet Bob and knows that Bob chooses the first
							bar more often, no matter to which extent, she must always choose the first
							bar! Moreover, the model’s recommendation to choose the first bar with
							the probability that she meets Bob in the first bar if she does not choose the
							first bar more often than usual sounds at least surprising and suggests that
							the model is wrong
						</aside>
					</section>
					<section>
						Seaman
					</section>
				</section>
				<section>
					<section>
							Discussion
					</section>
					<section>
						<ul>
							<li  class="fragment fade-in">The agent’s reasoning about preferred future behavior, can be formulated as Bayesian inference</li>
							<li class="fragment fade-in">Probability distributions can be used to specify all aspects of uncertainty or
							stochasticity, including agent preferences</li>
							<li class="fragment fade-in">Rewards can be interpreted as log-odds of stochastic preferences and do not need to be explicitly introduced.</li>
							<li class="fragment fade-in">Planning algorithms maximizing the expected utility find maximum a posteriori characterization of the rational policy distribution.</li>
						</ul>
						<aside class="notes">
							We demonstrated that reinforcement learning, that is, the agent’s reasoning
							about preferred future behavior, can be formulated as Bayesian inference.
							Probability distributions can be used to specify all aspects of uncertainty or
							stochasticity, including agent preferences. Rewards can be interpreted as
							log-odds of stochastic preferences and do not need to be explicitly introduced.
							Planning algorithms maximizing the expected utility find maximum
							a posteriori characterization of the rational policy distribution.
						</aside>
					</section>
			</section>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});
		</script>
	</body>
</html>
