<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h2>Reasoning About Future With Probabilistic Programs</h2>
					<p>Thesis defence</p>
					<p>Tomer Dobkin under the supervision of Dr. David Tolpin</p>
				</section>
				<section>
					<section>Probabilistic Programming</section>
					<section>
						<h3>Probabilistic Program Example</h3>
						<img src="assets/figures/coin-psaudo.png"></img>
						<aside class="notes">
							We have here a simple example of probablistic program.
							running forward - this sampling points of y
							running infernce (backward) over observation - we compute the posterior of theta
  						</aside>
					</section>
					<section>
					  <h3>Probabilistic Program Example</h3>
						  <img src="assets/figures/survey-psaudo.png"></img>
							<aside class="notes">
								We have here a more complex example of probablistic program.
								Condecut a survey of how much are satifies with his salery,
								without knowing each indivdual answers.
								We tell them:
								1. Toss a coin, if it is true:
								2. Answer the your true opinoin
								3. If false - toss again - and anser true iff Head
	  						</aside>
					</section>
				</section>
				<section>
					<section>
						The Fable of Bob and Alice
					</section>
					<section>
						<blockquote>
							Bob and Alice want to meet in a bar, but Bob left his phone
							at home. There are two bars, which Bob and Alice visit with
							different frequencies. Which bar Bob and Alice should head if
							they want to meet?
						</blockquote>
						<aside class="notes">
							We have here a simple example of probablistic program.
							running forward - this sampling points of y
							running infernce (backward) over observation - we compute the posterior of theta
  					 </aside>
					</section>
					<section>
						<img src="assets/figures/bob-alice-intro-1.png"></img>
						<aside class="notes">
							We have here a simple example of probablistic program.
							running forward - this sampling points of y
							running infernce (backward) over observation - we compute the posterior of theta
  					 </aside>
					</section>
					<section>
						<img src="assets/figures/bob-alice-intro-epsiode-infer.png"></img>
						<aside class="notes">
							We have here a simple example of probablistic program.
							running forward - this sampling points of y
							running infernce (backward) over observation - we compute the posterior of theta
  					 </aside>
					</section>
				</section>
				<section>
					<section>
						Determenistic Prefernces
					</section>
					<section>
						The fable of Bob and Alice is underspecified: we know how strong Bob
						or Alice prefer the first bar over the second one, or vice versa, however,
						we do not know how strong Bob and Alice prefer to meet (or avoid) each
						other.
					</section>
					<section>
						 <ol>
							 <li>$r_1$ - the reward for visiting the first bar;</li>
							 <li>$r_2$ - the reward for visiting the second bar;</li>
							 <li>$r_m$ - the reward for meeting the other person.</li>
						 </ol>
					</section>
					<section>
						Alice prefers the first bar ($r_{Alice}^1 > r_{Alice}^2$) and is
						rational, she should always go to the first bar.
						If, in addition, Alice ascribes reward $r_{Alice}^m$ to meeting Bob,
						and believes that Bob goes to the first bar with
						probability $\hat p_{Bob}^1$, then she should go to the first
						bar if $r_{Alice}^1 + \hat p_{Bob}^1r_{Alice}^m >
						r_{Alice}^2 + (1 - \hat p_{Bob}^1)r_{Alice}^m$, and to
						the second bar otherwise,
					</section>
					<aside class="notes">
						We have here a simple example of probablistic program.
						running forward - this sampling points of y
						running infernce (backward) over observation - we compute the posterior of theta
					</aside>
					<section>
						The softmax agent was proposed and is broadly used for
						modeling `approximately optimal agents`.
						<aside class="notes">
							However, stochastic behavior is not unusual, and we should be
							able to model it. To reconcile rewards and stochastic action
							choice, the softmax agent was proposed and is broadly used for
							modeling `approximately optimal agents'.
							A softmax agent
							chooses actions stochastically, with probabilities proportional
							to the exponentiated action utilities.
  						</aside>
					</section>
					<section>
						$r_{Alice}^1$ and $r_{Alice}^2$ such that $r_{Alice}^1 =
						r_{Alice}^2 + \log\left(\frac {p_{Alice}^1} {1 -
						p_{Alice}^1}\right)$. Concretely, for $p_{Alice}^1 = 0.55$ we
						obtain $r_1 \approx r_2 + 0.2$; only the difference between
						rewards matters in a softmax agent, rather than their absolute
						values.
					</section>
						<section>
							<img src="assets/figures/determinisic-1.png"></img>
						</section>
				</section>
				<section>
					<section>
					Stochastic Prefernces
					</section>
					<section>
						<img src="assets/figures/stochastic-1.png"></img>
					</section>
					<section>
						<img src="assets/figures/stochastic-2.png"></img>
					</section>
					<section>
							<img src="assets/figures/stochastic-3.png"></img>
					</section>
					<section>
						A direct corollary from (5.1) and (5.2) is that planning as inference
						with probabilistic preferences can be encoded as a Bayesian
						generative model using probabilities only, without resort to
						rewards.
					</section>
					<section>
						<blockquote>
							Preferences are naturally probabilistic in general and should be
							expressed as probabilities of choices under clairvoyance.
							`Deterministic` specification of preferences through rewards is
							an indirect way to encode probabilistic preferences. Reward
							maximization algorithms for policy search are in fact
							maximum a posteriori approximations of the
							distributions of optimal behaviors, which are inherently
							stochastic.
						</blockquote>
					</section>
				</section>
				<section>
					<section>
						Reinformencement Learning as Infernce
					</section>
					<section>
							<img src="assets/figures/mdp-1.png"></img>
					</section>
					<section>
							<img src="assets/figures/mdp-2.png"></img>
					</section>
					<section>
							<img src="assets/figures/mdp-3.png"></img>
					</section>
					<section>
							<img src="assets/figures/mdp-am.png"></img>
					</section>
				</section>
				<section>
					<section>
						Empirical Evaluation
					</section>
					<section>
						<h2>Bob and Alice Want to Meet - perfer same bar</h2>
						<img src="assets/figures/co-operative-1.png"></img>
						<aside class="notes">
							The original case, both players want to meet, we can see that it start from prior
							and rises to 0.83 and sty there (when p is 0.9)
						</aside>
					</section>
					<section>
						<h2>Bob and Alice Want to Meet - prefer different bars</h2>
						<img src="assets/figures/different-bars-1.png"></img>
					</section>
					<section>
						<h2>Bob chases Alice, Alice Avoids Bob</h2>
						<div class="r-stack">
							<img src="assets/figures/bob-chases-1.png" class="fragment fade-in-then-out"></img>
							<img src="assets/figures/bob-chases-2.png" class="fragment fade-in-then-out"></img>
						</div>
					</section>
					<section>
						<h2>Bob and Alice Avoid Each Other</h2>
						<div class="r-stack">
							<img src="assets/figures/both-avoid-1.png" class="fragment fade-in-then-out"></img>
							<img src="assets/figures/both-avoid-2.png" class="fragment fade-in-then-out"></img>
						</div>
					</section>
					<section>
						<h2>Alice learns Bob's Prefernces</h2>
						<img src="assets/figures/bob-alice-learned.png"></img>
					</section>
					<section>
						The Sailing Problem
					</section>
					<section>
							<img src="assets/figures/sailing-fig.png"></img>
					</section>
					<section>
							<img src="assets/figures/sailing-params.png"></img>
					</section>
					<section>
							<img src="assets/figures/sailing-model.png"></img>
					</section>
					<section>
							<img src="assets/figures/sailing-results.png"></img>
					</section>
					<section>
							<img src="assets/figures/sailing-results-2.png"></img>
					</section>
				</section>
				<section>
					<section>
						Related Work
					</section>
					<section>
						agentmodels
					</section>
					<section>
						wingate
					</section>
					<section>
						Seaman
					</section>
				</section>
				<section>
					<section>
							Discussion
					</section>
					<section>
						<ul>
							<li  class="fragment fade-in">The agent’s reasoning about preferred future behavior, can be formulated as Bayesian inference</li>
							<li class="fragment fade-in">Probability distributions can be used to specify all aspects of uncertainty or
							stochasticity, including agent preferences</li>
							<li class="fragment fade-in">Rewards can be interpreted as log-odds of stochastic preferences and do not need to be explicitly introduced.</li>
							<li class="fragment fade-in">Planning algorithms maximizing the expected utility find maximum a posteriori characterization of the rational policy distribution.</li>
						</ul>
						<aside class="notes">
							We demonstrated that reinforcement learning, that is, the agent’s reasoning
							about preferred future behavior, can be formulated as Bayesian inference.
							Probability distributions can be used to specify all aspects of uncertainty or
							stochasticity, including agent preferences. Rewards can be interpreted as
							log-odds of stochastic preferences and do not need to be explicitly introduced.
							Planning algorithms maximizing the expected utility find maximum
							a posteriori characterization of the rational policy distribution.
						</aside>
					</section>
			</section>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});
		</script>
	</body>
</html>
