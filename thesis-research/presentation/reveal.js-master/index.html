<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h2>Reasoning About Future With Probabilistic Programs</h2>
					<p>Thesis defence</p>
					<p>Tomer Dobkin under the supervision of Dr. David Tolpin</p>
				</section>
				<section>
					<ul>
						<li>Background</li>
						<li>The Fable of Bob and Alice</li>
						<li>Determenistic Preferences</li>
						<li>Stochastic Preferences</li>
						<li>Reinformencement Learning as Infernce</li>
						<li>Related Work</li>
						<li>Discussion</li>
					</ul>
				</section>
				<section>
					<section>
						Background
					</section>
					<section>Probabilistic Programming
					<aside class="notes">
						The main idea of probabilistic programming is to take the power of programming languages (expressivity) and design probabilistic generative models and make a generic inference backend to infer those models.
					The domain of probabilistic programming is the intersection between AI, programming languages, and statistical inference.
					</aside>
					</section>
					<section>
						<h3>Probabilistic Program Example</h3>
						<img src="assets/figures/coin-psaudo.png"></img>
						<aside class="notes">
							We can look at two ways to "run" the generative model.
							When you define a probalistic program you define a simulator
							With some latent varibles, that starting with some prior.
							This is how the data looks like, and you want to learn and update
							the posterior given some evidence
							We have here a simple example of probablistic program.
							running forward - this sampling points of y
							running infernce (backward) over observation - we compute the posterior of theta
							P(\theta | Y).
							and later we can use to sample from the posterior.
							new y's maybe.
  						</aside>
					</section>
					<section>
					  <h3>Probabilistic Program Example</h3>
						  <img src="assets/figures/survey-psaudo.png"></img>
							<aside class="notes">
								We have here a more complex example of probablistic program.
								Condecut a survey of how much are satifies with his salery,
								without knowing each indivdual answers.
								We tell them:
								1. Toss a coin, if it is true:
								2. Answer the your true opinoin
								3. If false - toss again - and anser true iff Head
	  						</aside>
					</section>
					<section>
							<img src="assets/figures/posterior-update-animation.gif"></img>
					</section>
					<section>
							<img src="assets/figures/bernoulli-update.gif"></img>
					</section>
					<section>
						Planning as Inference
						<aside class="notes">
							Planning, as a discipline of artificial intelligence, considers
							agents acting in environments. The agents have
							beliefs about their environments and perform actions which bring
							them rewards (or regrets). AI planning is concerned with
							algorithms that search for policies --- mappings from
							agents' beliefs about the environment to their actions. In
							planning-as-inference approach, policy search
							is expressed as inference in an appropriate probabilistic model.
						</aside>
					</section>
						<section>
							<p>policy: $\pi_\theta(E)$</p>
							<p class="fragment fade-in">latent variables: $\theta$, environment: $E$</p>
							<p class="fragment fade-in">reward: $r \sim \pi_\theta(E)$</p>
							<p class="fragment fade-in">The posterior distribution of policy parameters: $p(\theta\vert E)$</p>
							<p class="fragment fade-in">$p(\theta\vert E) \propto p(\theta)\exp\left(\mathbb{E}\left[r \sim \pi_\theta(E)\right]\right)$</p>
							<p class="fragment fade-in">Posterior inference on the above gives a distribution of policy parameters</p>
							<aside class="notes">
								A probabilistic program reifying the planning-as-inference
								approach encodes instantiation of policy $\pi_\theta(E)$,
								depending on latent parameters $\theta$, in environment $E$. In
								the course of execution, the program computes the reward $r \sim
								\pi_\theta(E)$, stochastic in general.  The posterior
								distribution of policy parameters $p(\theta\vert E)$, conditioned on
								the environment, is defined in terms of their prior distribution
								$p(\theta)$ and of the expected reward:
								Posterior inference on \eqref{eqn:planning-as-inference} gives a
								distribution of policy parameters, with the mode of the
								distribution corresponding to the policy maximizing the expected
								reward.
							</aside>
						</section>
					<section>
						Stochastic Conditioning
					</section>
					<section>
						<p>$p(x\vert y=y_0)$</p>
						<p class="fragment fade-in">$p(x\vert y \sim D_0)$</p>
						<p class="fragment fade-in">The objective is to infer $p(x\vert y \sim D)$ the distribution of $x$ given $D$</p>
						<p class="fragment fade-in">rather than an individual observation $y$</p>
						<img src="assets/figures/stochastic-notation.png" class="fragment fade-in"></img>

						<aside class="notes">
							Stochastic conditioning extends
							deterministic conditioning $p(x\vert y=y_0)$,
							i.e.conditioning on some random variable in our program $y$
							taking on a particular value $y_0$, to conditioning $p(x\vert y
							\sim D_0)$ on $y$ having the marginal distribution $D_0$.

							Unlike in the usual setting, the objective is to infer $p(x\vert y
							\sim D)$, the distribution of $x$ given distribution
							$D$, rather than an individual observation $y$.
							By convention, in statistical notation $y \sim D$ is placed
							above a rule to denote that distribution $D$ is observed through
							$y$ and is otherwise unknown to the model
						</aside>
					</section>
					<section>
						Epistemtic Reasoning
					</section>
					<section>
						<p>Epistemic reasoning, also known as theory of mind, is mutual reasoning of multiple agents about each others' beliefs and intentions.</p>
						<p class="fragment fade-in">Each agent's policy is mutually conditioned on other agents' policies</p>
						<p class="fragment fade-in">Probabilistic programming allows natural representation of epistemic reasoning</p>
						<p class="fragment fade-in">Inference in multiagent settings requires nested conditioning</p>
						<p class="fragment fade-in">The recursion can unroll indefinitely -> recursion depth is bounded by a constant.</p>
						<aside class="notes">
							Epistemic reasoning, also known as theory of
							mind, is mutual reasoning of multiple agents about each others'
							beliefs and intentions. Epistemic reasoning comes up in
							the planning-as-inference paradigm when each agent's policy is
							mutually conditioned on other agents' policies.
							Probabilistic programming allows natural representation of
							epistemic reasoning: the program modeling an agent invokes
							inference on the programs modeling the rest of the agents.
							However, this means that inference in multiagent settings
							requires nested conditioning, which is computationally
							challenging in general, although attempts are being made to find
							efficient implementations of nested conditioning in certain
							settings.  Since each agent's model recursively
							refers to models of other agents, the recursion can unroll
							indefinitely.  In a basic approach, the
							recursion depth is bounded by a constant.
						</aside>
					</section>
				</section>
				<section>
					<section>
						The Fable of Bob and Alice
						<aside class="notes">
							The use case we will explore in the research
						</aside>
					</section>
					<section>
						<blockquote>
							Bob and Alice want to meet in a bar, but Bob left his phone
							at home.<br>
							There are two bars, which Bob and Alice visit with different frequencies.<br>
							Which bar Bob and Alice should head if they want to meet?
						</blockquote>
						<aside class="notes">
							classic example from game of theory research,
							Many different settings can be considered based on this story.
							Bob and Alice may know each other's preferences with respect to
							the bars and to the meeting with the other person, be uncertain
							about the preferences, or hold wrong beliefs about the
							preferences.  Their preferences may be collaborative or adversarial (Bob wants to meet Alice, but Alice avoids.
							Bob may consider Alice's deliberation about Bob's
							behavior, and vice versa, recursively.
							It turns out that all these scenarios can be represented by a single model of
							interacting agents.  However, doing this properly, from the
							viewpoint of both specification and inference, requires certain
							care.
  					 </aside>
					</section>
					<section>
						<img src="assets/figures/bob-alice-intro-1.png"></img>
						<aside class="notes">
							Let's try to model it abstractly,
							Details of conditioning and inference set aside, the overall
							structure of the model is more or less obvious. There are two
							thought and action models, for each of the agents. The
							generative models have the same structure, but apparently
							different parameters.
							Explain line by line.
							$D_s$ is intentionally kept vague.
							Interaction between Bob and Alice is simulated by running
							Inference in each of the models and then by performing an
							action following (deterministically or stochastically) from
							inference results. In an episodic game, the model is conditioned
							on $\theta'$ and $\mathbb{I}_s$ for each agent, and the distributions of
							agents' actions are inferred. Then the move is simulated by
							drawing a sample from each of the posterior action
							distributions
  					 </aside>
					</section>
					<section>
						<img src="assets/figures/bob-alice-intro-epsiode-infer.png"></img>
						<aside class="notes">
						Since in a single episode there is no earlier evidence about the other agent's preferences, $\theta'$ is taken to be
						known, and $\tau'$ has no effect.
						Bob and Alice can even engage in a multiround game, in which
						each agent updates his or her own beliefs about the other
						agent's preferences based on the observed actions. To update an
						agent's belief about the other agent's preferences, the model is
						conditioned on an observed action $a$ and success $\mathbb{I}_s$, and
						$\theta'$ is inferred, and is later used to choose an action in the
						next round.

						A few ways to specify the agents and perform inference were
						proposed. We believe that some of them are wrong,  and others
						can be streamlined. In what follows, we propose a purely
						Bayesian generative approach to reasoning about future in
						multi-agent environments.
  					 </aside>
					</section>
				</section>
				<section>
					<section>
						Determenistic Preferences
					</section>
					<section>
						The fable of Bob and Alice is underspecified
						<aside class="notes">
							We know how strong Bob
							or Alice prefer the first bar over the second one, or vice versa, however,
							we do not know how strong Bob and Alice prefer to meet (or avoid) each
							other.
							We think that the study case is not specified enough,
							what is the meaning of this proablities, if in the end
							the most important mission is that they will meet,
							ofcourse if we talking about probablity greater 0.5
							for both of them for the same bar this is the rational thing to do
  					 </aside>
					</section>
					<section>
						 <ol>
							 <li>$r_1$ - the reward for visiting the first bar;</li>
							 <li>$r_2$ - the reward for visiting the second bar;</li>
							 <li>$r_m$ - the reward for meeting the other person.</li>
						 </ol>
						 <aside class="notes">
 							To add specification and formalism we introduce 3 rewards
							for each one of them. With this we can quantify their "intersts"
   					 </aside>
					</section>
					<section>
						<p class="fragment fade-in">$r_{Alice}^1 > r_{Alice}^2$</p>
						<p class="fragment fade-in">$r_{Alice}^1 + \hat p_{Bob}^1r_{Alice}^m >
						r_{Alice}^2 + (1 - \hat p_{Bob}^1)r_{Alice}^m$</p>

						<aside class="notes">
							Alice prefers the first bar ($r_{Alice}^1 > r_{Alice}^2$) and is
							rational, she should always go to the first bar.
							If, in addition, Alice ascribes reward $r_{Alice}^m$ to meeting Bob,
							and believes that Bob goes to the first bar with
							probability $\hat p_{Bob}^1$, then she should go to the first
							bar if $r_{Alice}^1 + \hat p_{Bob}^1r_{Alice}^m >
							r_{Alice}^2 + (1 - \hat p_{Bob}^1)r_{Alice}^m$, and to
							the second bar otherwise,
						</aside>
					</section>
					<section>
						<p class="fragment fade-in">$r_{Alice}^1 = r_{Alice}^2 +
							 \log\left(\frac {p_{Alice}^1} {1 - p_{Alice}^1}\right)$</p>
						<p class="fragment fade-in">$p_{Alice}^1 = 0.55$</p>
						<p class="fragment fade-in">$r_1 \approx r_2 + 0.2$</p>
					<aside class="notes">
						$r_{Alice}^1$ and $r_{Alice}^2$ such that $r_{Alice}^1 =
						r_{Alice}^2 + \log\left(\frac {p_{Alice}^1} {1 -
						p_{Alice}^1}\right)$. Concretely, for $p_{Alice}^1 = 0.55$ we
						obtain $r_1 \approx r_2 + 0.2$; only the difference between
						rewards matters in a softmax agent, rather than their absolute
						values.
						The softmax agent was proposed and is broadly used for
						modeling `approximately optimal agents`.
						However, stochastic behavior is not unusual, and we should be
						able to model it. To reconcile rewards and stochastic action
						choice, the softmax agent was proposed and is broadly used for
						modeling `approximately optimal agents'.
						A softmax agent
						chooses actions stochastically, with probabilities proportional
						to the exponentiated action utilities.
					</aside>
					</section>
						<section>
							<p class="fragment fade-in">$u_{Alice}^1 = r_{Alice}^1 + \hat p_{Bob}^1r_{Alice}^m$</p>
							<p class="fragment fade-in">$u_{Alice}^2 = r_{Alice}^2 + (1 - \hat p_{Bob}^1)r_{Alice}^m$</p>
							<p class="fragment fade-in">$\Pr(\mathbb{I}_{Alice}^1) = \frac {\exp(u_{Alice}^1)} {\exp(u_{Alice}^1) + \exp(u_{Alice}^2)}$</p>
							<!-- img src="assets/figures/determinisic-1.png"></img -->
							<aside class="notes">
								A softmax agent chooses actions stochastically, with probabilities proportional
								to the exponentiated action utilities, We will talk about
								why this is a good choice to use the softmax in the next chapter
							</aside>
						</section>
				</section>
				<section>
					<section>
					Probabilistic Preferences
					<aside class="notes">
						Previous section shows how stochastic behavior can be
						modeled using a softmax agent exhibiting an `approximately
						optimal' behavior, that is, selecting actions with
						log-probabilities proportional to their expected utilities.
						However, one may wonder what would be the optimal behavior if
						the behavior we model with softmax is only `approximately
						optimal' and hence suboptimal. We believe that treating
						stochastic behavior as suboptimal because it does not
						maximize the utility based on rewards we ascribe is
						contradictory. When Alice chooses the first bar 55\% of
						time and the second bar 45\% of time it is not because
						she cannot choose better (that is, always the first bar). It is
						her choice to sometimes go for a drink and have a lot of people
						around and loud music, sometimes go for a drink and have fewer
						people around and read a book without disturbing music in the
						background. Bob, in addition to going to one of the two bars to
						enjoy some drinks and see Alice, most probably eats breakfast
						every morning.  Let us speculate that sometimes Bob prefers
						scrambled eggs and sometimes porridge, randomly with probability
						about 0.6 towards scrambled eggs. However, if Bob had to eat
						scrambled eggs every morning and to give up on porridge entirely,
						because this is our perception of the optimal behavior based on
						rewards we ourselves introduced, rather arbitrarily, he would
						most probably be less happy about his diet.

						Assuming that there is no shortage of either oat or eggs, and
						that Bob's income essentially frees him from financial
						considerations in choosing one meal over the other, we should
						perceive Bob's behavior with respect to either the breakfast
						menu or the choice of a bar as optimal, and suggest means for
						specifying optimal stochastic behaviors and reasoning about
						them. One option is to postulate that softmax action selection
						is indeed optimal.  However, this option involves an assumption
						that softmax is a law of nature, and that there are latent
						rewards which we can only observe as choice probabilities
						through the softmax distribution. We believe that a separate
						theory of softmax-optimal stochastic behavior is unnecessary,
						and one can reason about agent behavior, both in single-agent
						and multi-agent setting, using Bayesian probabilities only. The
						rest of this section is an elaboration of this attitude.
					</aside>
					</section>
					<section>
						<img src="assets/figures/stochastic-1.png"></img>
						<aside class="notes">
							Let us return to the fable. According to the fable, the optimal
							probability of Alice's choice with respect to the bars is given
							(observed with high confidence or proclaimed by an oracle). To
							complete the formulation, it remains to specify the optimal
							probability of Alice to meet Bob. The question is: the probability of
							what event reflects the preference?  It turns out that we need
							to specify the probability of Alice choosing to visit the bar
							where Bob is heading, provided  Alice knows Bob's plans and
							would otherwise visit either bar with equal probability. Indeed,
							this results in the following generative model.----
							The model, in a superficially confusing way, generates Bob’s location I1
							Bob
							based on Alice’s desire to meet Bob. However, the confusion can be resolved
							by the following interpretation: when Alice chooses to go to the first bar, it
							is because, in addition to the slight preference over the second bar (line 1),
							she believes that Bob will also go to the first bar with probability pmA
							lice. It
							is Alice’s anticipations of Bob’s behavior that are generated by the model,
							rather than events.
						</aside>
					</section>
					<section>
						<img src="assets/figures/stochastic-2.png"></img>
						<aside class="notes">
							Since Model 3 generates anticipations, it should also be conditioned on
							anticipations. Let us, at this point, accept as a fact that Alice concludes that
							Bob goes to the first bar with probability ˆ p1
							Bob. Then, the model must be
							conditioned on I1
							Bob being distributed as Bernoulli( ˆ p1
							Bob). This involves an
							application of stochastic conditioning
						</aside>
					</section>
					<section>
							<img src="assets/figures/stochastic-3.png"></img>
							<aside class="notes">
								A connection between probabilistic preferences and deterministic
								preferences along with the Boltzmann distribution is revealed by
								writing down the analytical form of the probability distribution
							</aside>
					</section>
					<section>
						Planning as inference with probabilistic preferences can be encoded as a Bayesian
						generative model using probabilities only, without resort to
						rewards.
						<aside class="notes">
							A direct corollary from (5.1) and (5.2) is that Planning as inference with probabilistic preferences
							can be encoded as a Bayesian generative model using probabilities only, without resort to
							rewards
						</aside>
					</section>
					<section>
						<blockquote>
							Preferences are naturally probabilistic in general and should be
							expressed as probabilities of choices under clairvoyance.<br><br>
							`Deterministic` specification of preferences through rewards is
							an indirect way to encode probabilistic preferences.
						</blockquote>
						<aside class="notes">
							Reward maximization algorithms for policy search are in fact
							maximum a posteriori approximations of the
							distributions of optimal behaviors, which are inherently
							stochastic.
							may raise a question of the role of reward maximization in cases where
							rewards are given, such as games or trade. However, rewards in
							games or trade are themselves invented by humans because of
							difficulty apprehending probabilistic preferences. Once the
							essence of probabilistic preferences is understood, settings in
							which reward maximization is the optimal strategy lose their
							importance as general models and become mere edge cases.
						</aside>
					</section>
				</section>
				<section>
					<section>
						Reinformencement Learning as Infernce
						<aside class="notes">
							After we have seen a formalization let's try to connect it to the bigger picutre,
							Let us map certain well-known research problems of reinforcement
							learning to Bayesian inference. Some of the problems are perceived as
							having paradoxical properties, however, their paradoxicality is
							resolved by reformulation in terms of probabilities, without
							recourse to artificially introduced rewards and utilities.

							In the rest of the section, we provide an alternative, purely
							probabilistic, definition of the model known as Markov decision
							process (including the `partially observable' variant). Then, we
							formalize the tasks of planning and apprenticeship learning.
							There is an ambiguity in reinforcement learning literature with
							regard to notions of reinforcement learning, inverse
							reinforcement learning, apprenticeship learning, and planning. In
							this work, we chose to use the term planning to denote
							inferring the (optimal or rational) agent behavior given the
							model, and apprenticeship learning to denote inferring
							the model parameters given observed agent behavior.  Note that
							the notion of 'optimal' behavior is different here from the
							behavior considered optimal in traditional reinforcement
							learning literature: rather than corresponding to the mode of
							the posterior distribution of behaviors specified by the model,
							it is the posterior distribution itself.
						</aside>
					</section>
					<section>
							<img src="assets/figures/mdp-1.png"></img>
							<aside class="notes">
								The above definition of MDP provides a fertile ground  for
								reinforcement learning research, however, it mixes the agent, the
								environment, the goal of the agent in the environment, and the
								issues of practical computability, all in a single definition. In
								particular, the space of states $S$ and actions $A$ describe the
								agent (where it is and what it can do), the transition
								probabilities $T$ is a property of the environment, and the reward
								function $R$ defines the goal of the agent in the environment.
								The discount factor $\gamma$ modifies the environment in
								such a way that the length of the episode trajectory is
								geometrically distributed, through implicitly modifying $S$ and
								$T$ such that $S$ includes a terminal state and the transition
								matrix includes transition from any state to the terminal state with
								probability $1-\gamma$ --- and that ensures that the policy
								value is bounded. The geometrical distribution is not the only
								and not necessarily the most justified choice for trajectory
								length distribution. The Poisson or the negative-binomial
								distribution may, for example, be used instead when appropriate
								(for example, the negative-binomial distribution is a natural
								model for an agent repeatedly performing an action, with a
								certain number of failed attempts allowed). However,
								conventional MDP does not provide for such flexibility.

								Bayesian modeling allows instead to define a Markov decision
								process such that the agent, the environment, the agent
								preferences, and the computability issues are explicit and
								clearly separated. Before we proceed to our definition, let us draw
								the connection between the fable of Bob and Alice and a single-agent
								decision process. It is relatively easy to see how
								the case of two agents can be extended to a greater number of
								agents. However, a two-agent setup can as well be used naturally
								for modeling a single agent acting in a stochastic environment
								--- by representing the stochasticity of the environment as the
								other, neutral (neither adversarial nor collaborative) agent,
								acting regardless of the state of the first agent. We will stick
								to this paradigm here and define a 2-agent Markov
								decision process, with a single-agent Markov decision process as
								a special case of interaction of two agents in an environment
								with the other agent being neutral.

								An MDP invokes stochastic agents represented by their models $m_1$ and $m_2$
							</aside>
					</section>
					<section>
							<img src="assets/figures/mdp-2.png"></img>
							<aside class="notes">
								Action $a$ applied to the state is a composition $a_1 \circ a_2$
								of actions $a_1$ and $a_2$ chosen by each agent. How exactly
								the composition is accomplished is specific to a particular
								MDP instance. An MDP episode terminates when the terminal state
								$\bot$ is reached
							</aside>
					</section>
					<section>
							<img src="assets/figures/mdp-3.png"></img>
							<aside class="notes">
								an agent model represents reasoning behind action
								selection by the agent. Both agent models have the same
								structure, and define each agent's preferences and beliefs about
								the other agent.
								The rule between the first line and the rest of the
								model means that the model is stochastically conditioned on the
								distribution of $a_b$ (see
								Section~\ref{sec:stochastic-conditioning}). The last line of the
								model states that unless $s'$ is the terminal state, the model
								is recursively conditioned in state $s'$.

								Note that $\widehat {D_b}$  depends on both $s$ and $a_a$ in
								Definition~\ref{dfn:agent}. This
								addresses a sequential setting, in which agent $b$ chooses an
								action after observing the action of agent $a$. In a
								simultaneous setting, neither agent knows the other agent's
								choice, and the choice of an action depends on the current state
								only (formally, $\bot$ is passed as the second argument of
								$\widehat {D_b}$).
								Conventional planning consists in finding a policy that
								maximizes the expected total (discounted) reward. In the
								probabilistic formulation of Section~\ref{sec:mdp}, a policy of
								an agent that maximizes the expected total reward would correspond to
								marginal, with respect to the other agent,  maximum a
								posteriori (MMAP) trajectories. Obviously, MMAP trajectories
								are just an approximation of the behavior specified by the
								agent, reasonably good if the distribution is sharply peaked
								around the MMAP, but can be arbitrarily bad in general, as the
								eggs or porridge for breakfast dilemma.
								The rational behavior of the agent is specified by the agent
								model, and is in general stochastic (a
								distribution of actions in each state). From the Bayesian
								probabilistic standpoint, planning is just instantiation of the
								distribution specified by the agent model. Common inference
								algorithms, such as Markov chain Monte Carlo methods or
								variational inference, can be used for representing or
								approximating the policy --- the posterior distribution
								of agent's actions.

								In the case of a neutral second agent, such that representing the
								environment noise, the posterior inference is straightforward.
								However, if the agents are either collaborative or
								adversarial, epistemic reasoning must be taken into account,
								resulting in potentially unbounded nesting of inference.
							</aside>
					</section>
					<section>
							<img src="assets/figures/mdp-am.png"></img>
							<aside class="notes">
								Apprenticeship learning (Inverse RL) is concerned with a
								situation in which the agent preferences are not given
								explicitly  however, there are observed MDP trajectories, from
								which the preferences can be inferred Within the Bayesian probabilistic framework, apprenticeship
								learning reduces to just swapping some latent and observed
								variables in the agent. Agent preferences
								are specified by $D_s$. Inference on essentially the same model
								, with a slight modification that $D_s$ is a
								random variable drawn from a prior, conditioned on MDP trajectories.
								actions are observed rather than drawn here.
							</aside>
					</section>
				</section>
				<section>
					<section>
						Empirical Evaluation
						<aside class="notes">
							We illustrate reasoning about future with probabilistic
							preferences using implementations of the fable of Bob and Alice
							and of the sailing problem.
							We implemented the model and inference in Gen.
							In the experiments below, we show results obtained both by
							Monte-Carlo inference on the Gen model and using the analytical
							model, both to demonstrate that the Gen model, more complicated
							but generalizable to a wide range of problems, returns
							essentially the same results as the analytical model and to help
							the reader discern easily between actual trends and
							approximation errors in inference outcomes. We consider several
							cases of Bob's and Alice's preferences. We run Monte Carlo
							inference for 5000 iterations with 10\% burn-in. Each plot shows
							the posterior choice distributions of each of the agents for a
							range of deliberation depths, with depth 0 corresponding to the prior
							behavior, depth 1 to the posterior using the prior behavior of
							the other agent, and so on.
							One of the challanges is to keep the nested sampling efficient.
							We did it using memoization (Dynamic Programming on each level)
						</aside>
					</section>
					<section>
						<h2>Bob and Alice Want to Meet - perfer same bar</h2>
						<img src="assets/figures/co-operative-1.png"></img>
						<aside class="notes">
							The original case, both players want to meet, we can see that it start from prior
							and rises to 0.83 and sty there (when p is 0.9), prefer the same bar
						</aside>
					</section>
					<section>
						<h2>Bob and Alice Want to Meet - prefer different bars</h2>
						<img src="assets/figures/different-bars-1.png"></img>
						<aside class="notes">
							addresses contradictory
							preferences with respect to bars --- Alice strongly prefers the
							first bar, while Bob slightly prefers the second bar. Since they
							still want to meet, they both choose, when the deliberation
							depth is sufficient, the first bar more often than the second
							one.
						</aside>
					</section>
					<section>
						<h2>Bob chases Alice, Alice Avoids Bob</h2>
						<div class="r-stack">
							<img src="assets/figures/bob-chases-1.png" class="fragment fade-in-then-out"></img>
							<img src="assets/figures/bob-chases-2.png" class="fragment fade-in-then-out"></img>
							<aside class="notes">
								Another setting is an instance of the pursuit-evasion game: Bob
								chases Alice, but Alice avoids Bob. The choice distributions of
								both agents depend on how strong their preferences to meet, or
								to avoid the meeting, are.
								Figure~\ref{fig:ba-0.55-0.25-0.55-0.75}  shows inference results
								for the case where Bob's and Alice's feelings are opposite but
								mild. As Bob and Alice deliberate deeper and deeper, their
								choice distributions eventually converge to stable limits.
								However, if Bob's and Alice's feelings are opposite and strong
								(Figure~\ref{fig:ba-0.55-0.1-0.55-0.9}),
								the agents overthink and act impulsively --- as the deliberation
								depth increases, the choice distributions of both agents
								alternate between going to the first or to the second bar with
								increasing probability.
							</aside>
						</div>
					</section>
					<section>
						<h2>Bob and Alice Avoid Each Other</h2>
						<div class="r-stack">
							<img src="assets/figures/both-avoid-1.png" class="fragment fade-in-then-out"></img>
							<img src="assets/figures/both-avoid-2.png" class="fragment fade-in-then-out"></img>
							<aside class="notes">
								Further on, we explore the case when Bob and Alice avoid each
								other, that is, both of them would rather go to the pub where
								they are unlikely to meet the other person. When the preference
								towards avoiding is mild (Figure~\ref{fig:ba-0.55-0.25}), Bob
								and Alice, with sufficient deliberation depth, approach their
								prior preferences, that is they both go to the first bar with
								the probability approaching $p_{Alice}^1 = p_{Bob}^1$. Indeed,
								rationally this the best they can do to minimize their chances
								to meet each other without coordination, while still respecting
								their preference of the first bar.
								However, when Bob's and Alice's mutual despisal is too strong
							(Figure~\ref{fig:ba-0.55-0.05}), mutual epistemic reasoning does
							a poor job --- Bob and Alice switch their increasingly strong
							posterior preferences between the first and the second bar with
							each level of deliberation depth, similarly to the case of
							strong feelings in pursuit-evasion situation. One may argue that
							these outcomes are algorithmically anomalous, or `irrational';
							however, they may be also interpreted as a demonstration that
							rational behavior is not unconditionally stable, and cannot be
							guaranteed for arbitrary combinations of preferences.
							</aside>
						</div>
					</section>
					<section>
						<h2>Alice learns Bob's preferences</h2>
						<img src="assets/figures/bob-alice-learned.png"></img>
						<aside class="notes">
							Finally, let us see how Alice can find out whether Bob wants to
							meet her by observing Bob's behavior. We use the same model of
							Alice for inference, but instead of conditioning the model on
							Bob's preferences, we condition the model on the history of
							Bob's choices and infer Bob's preference (log-odds) of meeting
							Alice. We condition the model on three Bob's choices in a row,
							and compare Alice's conclusions for two cases:

							-Bob chose the first bar three times in a row;
							- Bob chose the second bar three times in a row.

							To visualize the results, we draw 100 samples from the posterior
							distribution of the probability with that Bob would choose to
							meet Alice everything else being equal.
							Figure~\ref{fig:ba-multiround} shows a two-dimensional scatter
							plot where each direction is the marginal distribution of
							\textit{log-odds} of Bob willing to meet Alice. Indeed, 96 out
							of 100 points are below the diagonal, meaning that Alice can be
							very confident that Bob is willing to meet her if she observes
							Bob in the first bar 3 times in a row. However, if Bob shows up
							three times in a row in the second bar, Alice should conclude
							that Bob is avoiding her
						</aside>
					</section>
					<section>
						The Sailing Problem
					</section>
					<section>
							<img src="assets/figures/sailing-fig.png"></img>
							<aside class="notes">
								The sailing problem (Figure 7.8) is a popular benchmark problem for search
								and planning. A sailing boat must travel between the opposite corners A
								and B of a square lake of a given size. At each step, the boat can head in 8
								directions (legs) to adjacent squares (Figure 7.8a). The unit distance cost of
								movement depends on the wind (Figure 7.8b), which can also blow in 8
								directions. There are five relative boat and wind directions and associated
								costs: into, up, cross, down, and away. The cost of sailing into the wind is
								prohibitively high, upwind is the highest feasible, and away from the wind
								is the lowest. The side of the boat off which the sail is hanging is called the
							</aside>

					</section>
					<section>
							<img src="assets/figures/sailing-params.png"></img>
							<aside class="notes">
							For any given lake size, there is a non-parametric stochastic policy that
							tabulates the distribution of legs for each combination of location, tack,
							and wind. However, such policy does not generalize well — if the lake
							area increases, due to a particularly rainy year for example, the policy is
							not applicable to the new parts of the lake. In this case study, we infer
							instead a generalizable parametric policy balancing between hesitation in
							anticipation for a better wind and rushing to the goal at any cost. The
							policy chooses a leg with the log-probability equal to the euclidian distance
							between the position after the leg and the goal, multiplied by the policy
							parameter q (the leg directed into the wind is excluded from choices). The
							greater the q, the higher is the probability that a leg bringing the boat closer
							to the goal will be chosen:
						</aside>
					</section>
					<section>
							<img src="assets/figures/sailing-model.png"></img>
							<aside class="notes">
								The first agent, Alice, is the boat. The agent chooses a path across the
							lake.
							• The second agent, Bob, is the wind. The agent chooses a random walk
							of wind direction along the boat’s path. The wind is a neutral agent—
							it does not try to either help or tamper with the boat.
							• The boat’s path, stochastically conditioned on the wind, has the prob
						</aside>
					</section>
					<section>
							<img src="assets/figures/sailing-results.png"></img>
						<aside class="notes">
								The model parameters (cost and wind change probabilities), same as
								in [KS06, TS12], are shown in Table 7.1. A non-informative improper prior
								is imposed on q. We fit the model using pseudo-marginal Metropolis-
								Hastings [AR09] and used 10 000 samples to approximate the posterior.
								Figure 7.9 shows the posterior distribution of the unit cost. Table 7.2 shows
								the expected travel costs, with the expectations estimated both over the unit
								cost and the wind. The inferred travel costs are compared to the travel costs
								of the ‘optimal’ policy and of the greedy policy, according to which the
								boat always heads in the direction of the steepest decrease of the distance
								to the goal. One can see that the inferred policy attains an expected travel
								cost lying between the greedy policy and the ‘optimal’ policy, as one would
								anticipate.
					</aside>
					</section>
					<section>
							<img src="assets/figures/sailing-results-2.png"></img>
					</section>
				</section>
				<section>
					<section>
						Related Work
						<aside class="notes">
							We will now see similar works, with mistakes
							on modelling. We started from their approach and found
							the mistakes, and fixed them using stochastic condtioning.
							Statistical models are most suited for reasoning about the
							present. Reasoning about the past or, in particular, the future
							is counterintuitive and hard to get right. Multi-agent planning as
							inference, exemplified by the fable about Bob and Alice,
							presents logical and statistical traps which are easy to fall
							into, often without even noticing. One common mistake is
							conditioning on a future event as on a present observation,
							which is related to the Newcomb's paradox; however, there are
							also other mistakes.  Incorrect treatments of the problem in the
							literature, presented in the rest of this section,  accentuate
							the need for a probabilistically sound Bayesian approach to
							planning as inference, the subject of this work.
						</aside>
					</section>
					<section>
						<h2>agentmodels</h2>
						<img src="assets/figures/agent-models-1.png"></img>
						<aside class="notes">
							Consider, for simplicity, one
							particular variant of the fable,
							in which Bob and Alice both generally choose the first bar with
							probability 55\% and the second bar with probability 45\% and want to
							meet. The agent model (of Alice, just
							to be concrete) combines the prior on Alice's behavior, which
							is the Bernoulli distribution $\mathbb{I}_{Alice}^1 \sim
							\mathrm{Bernoulli}(p_{Alice}^1=0.55)$, and the conditioning on Alice meeting
							Bob. Alice does not know Bob's location, but can reason about
							the distribution of Bob's choices $\mathrm{Bernoulli}(\hat
							p_{Bob}^1)$. $\hat p_{Bob}^1$ coincides with Bob's prior
							$p_{Bob}^1=0.55$ in the simplest case, but can be also
							influenced by Alice's consideration of Bob's reasoning about
							Alice, which is a case of epistemic reasoning
							The posterior probability of Alice going to the first bar
							according to
							is $\frac {0.55\hat p_{Bob}^1} {0.55\hat p_{Bob}^1 + 0.45(1-\hat p_{Bob}^1)}$. If
							$\hat p_{Bob}^1 > 0.5$, that is, if Alice maintains that Bob prefers
							the first bar, Alice will go to the first bar more often if she
							wants to meet Bob, with the probability in which it
							would meet Bob in the first bar if she would not adjust her
							behavior. It may seem (and it is indeed the course of reasoning
							in) that Alice makes a better
							choice thinking about Bob, and the more she thinks about Bob
							(willing to meet Alice and taking into consideration that Alice
							is willing to meet Bob, and so on) the higher is the probability
							of Alice to choose the first bar.

							However, on slightly more thorough consideration, Alice's
							deliberation is irrational. If Alice wants to meet Bob and knows
							that Bob chooses the first bar more often, no matter to which
							extent, she must always choose the first bar! Moreover, the
							model's recommendation to choose the first bar with the
							probability that she meets Bob in the first bar if she does
							not choose the first bar more often than usual sounds
							at least surprising and suggests that the model is wrong.

							Indeed, the above model suffers from two problems. First, it
							conditions on the future as though it were the present, that is
							as though Alice knew, in each case, Bob's choices. Second, the
							agents' preferences with respect to the choice of a bar and to
							meeting each other are expressed using different languages.
							Alice chooses the first or the second bar with a non-trivial
							probability, but wants to meet Bob non-probabilisticall}.
							Moreover, it is not immediately obvious whether and how a
							probability can be ascribed to a desire (rather than a future
							event). Mixing two incompatible formulations causes confusion
							and gives self-contradictory results.
						</aside>
					</section>
					<section>
						<h2>Nested Reasoning About Autonomous Agents Using Probabilistic Programs</h2>
						<img src="assets/figures/seaman.png"></img>
						<aside class="notes">
							Approach a pursuit-evasion problem, in the form of
							a chaser (sheriff) and a runner (thief) moving through city
							streets, with tools of probabilistic programming and planning as
							inference. A simpler but similar setting can be expressed using
							Bob and Alice with adversarial preferences with respect to
							meeting each other: Bob (the chaser) wants to meet Alice, but
							Alice (the runner) wants to avoid Bob.
							formulates the inference problem in terms of the
							Boltzmann distribution of trajectories based on the travel time
							of the runner, as well as the time the runner seen by the
							chaser, positive for the chaser (wants to see the runner as long
							as possible) and negative for the runner (wants to avoid being
							seen by the chaser as much as possible).  However, looking
							for efficient inference, this work proposes to replace nested
							conditioning by conditioning of each level of reasoning on a
							single sample from the previous level. This is, again,
							conditioning on the future as though the future were known, and
							leads to wrong results.

							The problem can be hard to realize on the complicated setting
							explored in the work, but becomes obvious on the example of Bob
							and Alice. Consider, for simplicity, the setting in which Alice
							has equal preferences regarding the bars and attributes reward 1
							to avoiding Bob and 0 to meeting him. Bob chooses the first bar
							in 55\% of cases; Alice employs a single level of epistemic
							reasoning. It is easy to see that the optimal policy for Alice
							is to always go to the second bar for the expected reward or 0.55.
							However, the mode of the posterior inferred using the algorithm
							in~\cite{SMW18} is for Alice to choose the second bar with
							probability of 0.55, for the expected reward of 1! This is
							because Alice's decision is (erroneously) conditioned on Bob's
							anticipated choice of a bar, and hence Alice pretends that she
							can always choose the other bar (which she cannot).

						</aside>
					</section>
				</section>
				<section>
					<section>
							Discussion
					</section>
					<section>
						<ul>
							<li  class="fragment fade-in">The agent’s reasoning about preferred future behavior, can be formulated as Bayesian inference</li>
							<li class="fragment fade-in">Probability distributions can be used to specify all aspects of uncertainty or
							stochasticity, including agent preferences</li>
							<li class="fragment fade-in">Rewards can be interpreted as log-odds of stochastic preferences and do not need to be explicitly introduced.</li>
							<li class="fragment fade-in">Planning algorithms maximizing the expected utility find maximum a posteriori characterization of the rational policy distribution.</li>
						</ul>
						<aside class="notes">
							We demonstrated that reinforcement learning, that is, the agent’s reasoning
							about preferred future behavior, can be formulated as Bayesian inference.
							Probability distributions can be used to specify all aspects of uncertainty or
							stochasticity, including agent preferences. Rewards can be interpreted as
							log-odds of stochastic preferences and do not need to be explicitly introduced.
							Planning algorithms maximizing the expected utility find maximum
							a posteriori characterization of the rational policy distribution.
						</aside>
					</section>
			</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});
		</script>
	</body>
</html>
